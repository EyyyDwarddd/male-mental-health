{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported the Following Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/edwardmendoza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/edwardmendoza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import stop_words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues = pd.read_csv('men_df_eda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mens_issues.drop(columns = ['Unnamed: 0', 'title', 'selftext', 'subreddit', 'author', 'post_length', 'all_text2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date resourc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vunrabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk away  women abort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mainstream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  all_text\n",
       "0  masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...\n",
       "1                                                                                                                                                                                             date resourc\n",
       "2                                                                                                                                                                                                  vunrabl\n",
       "3                                                                                                                                                                                   walk away  women abort\n",
       "4                                                                                                                                                                                               mainstream"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mens_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mens_issues.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26595</th>\n",
       "      <th>26596</th>\n",
       "      <th>26597</th>\n",
       "      <th>26598</th>\n",
       "      <th>26599</th>\n",
       "      <th>26600</th>\n",
       "      <th>26601</th>\n",
       "      <th>26602</th>\n",
       "      <th>26603</th>\n",
       "      <th>26604</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_text</th>\n",
       "      <td>masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...</td>\n",
       "      <td>date resourc</td>\n",
       "      <td>vunrabl</td>\n",
       "      <td>walk away  women abort</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>think  popular discours harm convers better</td>\n",
       "      <td>stop abort</td>\n",
       "      <td>stereotyp</td>\n",
       "      <td>anybodi check palgrav handbook male psycholog mental health thought read good sourc mental health hard shell euro imagin  check thought</td>\n",
       "      <td>group better help woman wife  girl licens profession counselor start youtub channel address mental health free laugh return read therapist help deepli hurt silent struggl love work strong pull rea...</td>\n",
       "      <td>...</td>\n",
       "      <td>grow multipl sister  fightget ampnbsp look ampnbsp think effect ampnbsp effemin result ampnbsp present relationship</td>\n",
       "      <td>want  help boyfriend learn respons boyfriend relationship deterior graduat univers finish spring lucki great program work coupl cours finish graduat summer  work field work labour laid issu doesnt...</td>\n",
       "      <td>enjoy oral girl</td>\n",
       "      <td>male equival flower  girl flower nice gestur girl</td>\n",
       "      <td>decis life turn unpleas success</td>\n",
       "      <td>romant intim andor sexual relationship honestli think chang</td>\n",
       "      <td>arrang date bisexu continu annoy girl simpli practic process arrang date imposs lose continu string girl repli text vagu unhelp answer moment girl  date went text follow shed great want meet chat ...</td>\n",
       "      <td>favorit podcast listen</td>\n",
       "      <td>respond hand written letter express silli simpl straightforward edit decid good clariti thank honesti</td>\n",
       "      <td>grow mustach clip hair center leav grow contribut outer  mustach look mustach advic sure vari depend style sure worthwhil topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                            0      \\\n",
       "all_text  masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...   \n",
       "\n",
       "                 1        2                        3           4      \\\n",
       "all_text  date resourc  vunrabl   walk away  women abort  mainstream   \n",
       "\n",
       "                                                5            6          7      \\\n",
       "all_text  think  popular discours harm convers better   stop abort  stereotyp   \n",
       "\n",
       "                                                                                                                                            8      \\\n",
       "all_text  anybodi check palgrav handbook male psycholog mental health thought read good sourc mental health hard shell euro imagin  check thought   \n",
       "\n",
       "                                                                                                                                                                                                            9      \\\n",
       "all_text  group better help woman wife  girl licens profession counselor start youtub channel address mental health free laugh return read therapist help deepli hurt silent struggl love work strong pull rea...   \n",
       "\n",
       "          ...  \\\n",
       "all_text  ...   \n",
       "\n",
       "                                                                                                                        26595  \\\n",
       "all_text  grow multipl sister  fightget ampnbsp look ampnbsp think effect ampnbsp effemin result ampnbsp present relationship   \n",
       "\n",
       "                                                                                                                                                                                                            26596  \\\n",
       "all_text  want  help boyfriend learn respons boyfriend relationship deterior graduat univers finish spring lucki great program work coupl cours finish graduat summer  work field work labour laid issu doesnt...   \n",
       "\n",
       "                     26597                                              26598  \\\n",
       "all_text   enjoy oral girl  male equival flower  girl flower nice gestur girl   \n",
       "\n",
       "                                      26599  \\\n",
       "all_text    decis life turn unpleas success   \n",
       "\n",
       "                                                                 26600  \\\n",
       "all_text   romant intim andor sexual relationship honestli think chang   \n",
       "\n",
       "                                                                                                                                                                                                            26601  \\\n",
       "all_text  arrang date bisexu continu annoy girl simpli practic process arrang date imposs lose continu string girl repli text vagu unhelp answer moment girl  date went text follow shed great want meet chat ...   \n",
       "\n",
       "                           26602  \\\n",
       "all_text  favorit podcast listen   \n",
       "\n",
       "                                                                                                          26603  \\\n",
       "all_text  respond hand written letter express silli simpl straightforward edit decid good clariti thank honesti   \n",
       "\n",
       "                                                                                                                                    26604  \n",
       "all_text  grow mustach clip hair center leav grow contribut outer  mustach look mustach advic sure vari depend style sure worthwhil topic  \n",
       "\n",
       "[1 rows x 26325 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = mens_issues.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaa</th>\n",
       "      <th>aaaaaaaaaahhhhhhhh</th>\n",
       "      <th>aaaaaaaaargh</th>\n",
       "      <th>aaaaaaaanyway</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aawww</th>\n",
       "      <th>ababa</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>카카오스토리</th>\n",
       "      <th>카톡mad</th>\n",
       "      <th>키워드광고</th>\n",
       "      <th>페이스북</th>\n",
       "      <th>페이스북광고</th>\n",
       "      <th>홈페이지만드는법</th>\n",
       "      <th>홈페이지제작방법</th>\n",
       "      <th>홈페이지제작사이트</th>\n",
       "      <th>홈페이지제작업체</th>\n",
       "      <th>홈페이지형블로그</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaaa  aaaaaaaaaahhhhhhhh  aaaaaaaaargh  aaaaaaaanyway  aardvark  aaron  \\\n",
       "0        0                   0             0              0         0      0   \n",
       "1        0                   0             0              0         0      0   \n",
       "2        0                   0             0              0         0      0   \n",
       "3        0                   0             0              0         0      0   \n",
       "4        0                   0             0              0         0      0   \n",
       "\n",
       "   aawww  ababa  aback  abandon  ...  카카오스토리  카톡mad  키워드광고  페이스북  페이스북광고  \\\n",
       "0      0      0      0        0  ...       0      0      0     0       0   \n",
       "1      0      0      0        0  ...       0      0      0     0       0   \n",
       "2      0      0      0        0  ...       0      0      0     0       0   \n",
       "3      0      0      0        0  ...       0      0      0     0       0   \n",
       "4      0      0      0        0  ...       0      0      0     0       0   \n",
       "\n",
       "   홈페이지만드는법  홈페이지제작방법  홈페이지제작사이트  홈페이지제작업체  홈페이지형블로그  \n",
       "0         0         0          0         0         0  \n",
       "1         0         0          0         0         0  \n",
       "2         0         0          0         0         0  \n",
       "3         0         0          0         0         0  \n",
       "4         0         0          0         0         0  \n",
       "\n",
       "[5 rows x 27756 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = mens_issues.index\n",
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x1258525e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words.stop_words)\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = mens_issues.index\n",
    "\n",
    "# Pickle it for later use\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"autoerotic\" + 0.004*\"catastoph\" + 0.004*\"coattail\" + 0.004*\"ablat\" + 0.004*\"battlefront\" + 0.003*\"chequ\" + 0.003*\"coars\" + 0.003*\"child\" + 0.003*\"afghanistan\" + 0.003*\"bigfram\"'),\n",
       " (1,\n",
       "  '0.003*\"gail\" + 0.003*\"bugaloo\" + 0.002*\"gona\" + 0.002*\"hatefil\" + 0.002*\"burr\" + 0.002*\"girl\" + 0.001*\"bezo\" + 0.001*\"garth\" + 0.001*\"electromagnet\" + 0.001*\"growingup\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"dehuman\" + 0.004*\"burr\" + 0.004*\"defenc\" + 0.004*\"anarchosyndicalist\" + 0.004*\"datingand\" + 0.003*\"earthsea\" + 0.003*\"attende\" + 0.003*\"fatigu\" + 0.003*\"tommi\" + 0.003*\"dickless\"'),\n",
       " (1,\n",
       "  '0.005*\"garth\" + 0.005*\"bezo\" + 0.005*\"girl\" + 0.005*\"hatefil\" + 0.004*\"cipralex\" + 0.004*\"girlsll\" + 0.003*\"grater\" + 0.003*\"gail\" + 0.003*\"girlfriendi\" + 0.003*\"exposur\"'),\n",
       " (2,\n",
       "  '0.010*\"gail\" + 0.009*\"growingup\" + 0.006*\"gamedev\" + 0.005*\"goddamn\" + 0.004*\"crowi\" + 0.004*\"friendsformerli\" + 0.004*\"catchuphello\" + 0.004*\"automod\" + 0.004*\"greatgrandmoth\" + 0.004*\"friendso\"'),\n",
       " (3,\n",
       "  '0.010*\"autoerotic\" + 0.006*\"child\" + 0.005*\"cismenmen\" + 0.005*\"dieseas\" + 0.005*\"deathb\" + 0.004*\"bigotrymisconcept\" + 0.004*\"centauri\" + 0.004*\"crazyi\" + 0.003*\"diddidnt\" + 0.003*\"abruptli\"'),\n",
       " (4,\n",
       "  '0.012*\"battlefront\" + 0.005*\"haphazardli\" + 0.004*\"havе\" + 0.004*\"finsta\" + 0.004*\"flat\" + 0.003*\"hasten\" + 0.003*\"dwarfism\" + 0.003*\"nonconsentu\" + 0.003*\"finac\" + 0.003*\"envioment\"'),\n",
       " (5,\n",
       "  '0.021*\"ablat\" + 0.017*\"chequ\" + 0.017*\"catastoph\" + 0.016*\"coattail\" + 0.015*\"bing\" + 0.015*\"biomechan\" + 0.014*\"close\" + 0.012*\"dissuad\" + 0.009*\"disrespectedmanipul\" + 0.009*\"bigfram\"'),\n",
       " (6,\n",
       "  '0.004*\"burglari\" + 0.004*\"bougi\" + 0.004*\"decis\" + 0.004*\"devout\" + 0.004*\"cutscen\" + 0.004*\"halfsleev\" + 0.004*\"commitissu\" + 0.004*\"outlast\" + 0.004*\"mannslib\" + 0.004*\"andi\"'),\n",
       " (7,\n",
       "  '0.032*\"coars\" + 0.024*\"gona\" + 0.010*\"battlefront\" + 0.008*\"coolcrazyweirdfun\" + 0.007*\"chin\" + 0.007*\"broswol\" + 0.005*\"curtli\" + 0.005*\"ball\" + 0.005*\"concuss\" + 0.005*\"cutlur\"'),\n",
       " (8,\n",
       "  '0.008*\"bugaloo\" + 0.004*\"gail\" + 0.003*\"gang\" + 0.003*\"foolhardi\" + 0.003*\"electromagnet\" + 0.002*\"finest\" + 0.002*\"beholden\" + 0.002*\"comatos\" + 0.002*\"heavili\" + 0.002*\"homework\"'),\n",
       " (9,\n",
       "  '0.020*\"banish\" + 0.009*\"argumentfight\" + 0.008*\"acept\" + 0.008*\"daughterinlaw\" + 0.007*\"ccvictimsskycom\" + 0.007*\"cuddli\" + 0.007*\"bloodbath\" + 0.007*\"afsp\" + 0.007*\"fetch\" + 0.006*\"anonymouslylik\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda2 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=5)\n",
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"coars\" + 0.006*\"battlefront\" + 0.003*\"gamedev\" + 0.003*\"haphazardli\" + 0.003*\"gail\" + 0.003*\"finsta\" + 0.002*\"gona\" + 0.002*\"excusedelus\" + 0.002*\"dwarfism\" + 0.002*\"garth\"'),\n",
       " (1,\n",
       "  '0.011*\"fancentro\" + 0.007*\"banish\" + 0.005*\"burr\" + 0.004*\"chin\" + 0.004*\"argumentfight\" + 0.003*\"goddamn\" + 0.003*\"ccvictimsskycom\" + 0.003*\"monthsold\" + 0.003*\"betray\" + 0.003*\"beholden\"'),\n",
       " (2,\n",
       "  '0.005*\"bugaloo\" + 0.004*\"gail\" + 0.002*\"hatefil\" + 0.002*\"finest\" + 0.002*\"electromagnet\" + 0.002*\"homework\" + 0.002*\"girl\" + 0.002*\"friendso\" + 0.002*\"foolhardi\" + 0.002*\"burr\"'),\n",
       " (3,\n",
       "  '0.011*\"bezo\" + 0.005*\"comichttpimgurcomaxm\" + 0.004*\"betasi\" + 0.004*\"centauri\" + 0.004*\"cuddli\" + 0.004*\"bloodbath\" + 0.003*\"fatigu\" + 0.003*\"afsp\" + 0.003*\"growingup\" + 0.003*\"bathroom\"'),\n",
       " (4,\n",
       "  '0.006*\"autoerotic\" + 0.005*\"catastoph\" + 0.005*\"coattail\" + 0.005*\"ablat\" + 0.004*\"chequ\" + 0.004*\"child\" + 0.004*\"afghanistan\" + 0.003*\"cismenmen\" + 0.003*\"bigfram\" + 0.003*\"blackpil\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=5)\n",
    "lda3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these passes of topic modeling via LDA, it's difficult to interpret the topics that we have here. Will adjust the words that will be passed through the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits to Alice Zhao\n",
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = mens_issues.all_text.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words.stop_words)\n",
    "data_cvn = cvn.fit_transform(mens_issues.all_text)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = mens_issues.all_text.index\n",
    "#data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"friend\" + 0.020*\"girl\" + 0.020*\"date\" + 0.017*\"work\" + 0.016*\"relationship\" + 0.014*\"life\" + 0.011*\"help\" + 0.009*\"start\" + 0.008*\"look\" + 0.006*\"school\"'),\n",
       " (1,\n",
       "  '0.039*\"women\" + 0.016*\"woman\" + 0.008*\"delet\" + 0.007*\"male\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.005*\"look\" + 0.005*\"issu\" + 0.005*\"attract\" + 0.005*\"gener\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"friend\" + 0.022*\"girl\" + 0.021*\"date\" + 0.018*\"relationship\" + 0.014*\"work\" + 0.012*\"life\" + 0.010*\"look\" + 0.009*\"start\" + 0.007*\"woman\" + 0.006*\"help\"'),\n",
       " (1,\n",
       "  '0.028*\"women\" + 0.012*\"help\" + 0.007*\"issu\" + 0.007*\"work\" + 0.007*\"life\" + 0.006*\"male\" + 0.005*\"health\" + 0.005*\"masculin\" + 0.005*\"posit\" + 0.004*\"problem\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 2\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"hair\" + 0.020*\"watch\" + 0.015*\"favorit\" + 0.014*\"look\" + 0.013*\"bodi\" + 0.012*\"movi\" + 0.010*\"porn\" + 0.010*\"video\" + 0.010*\"face\" + 0.007*\"style\"'),\n",
       " (1,\n",
       "  '0.076*\"girl\" + 0.074*\"date\" + 0.043*\"relationship\" + 0.042*\"women\" + 0.025*\"woman\" + 0.019*\"delet\" + 0.018*\"friend\" + 0.014*\"look\" + 0.013*\"attract\" + 0.012*\"partner\"'),\n",
       " (2,\n",
       "  '0.024*\"work\" + 0.007*\"studi\" + 0.007*\"colleg\" + 0.007*\"univers\" + 0.006*\"success\" + 0.006*\"graduat\" + 0.006*\"money\" + 0.006*\"career\" + 0.005*\"field\" + 0.005*\"school\"'),\n",
       " (3,\n",
       "  '0.030*\"women\" + 0.015*\"masculin\" + 0.012*\"male\" + 0.012*\"issu\" + 0.010*\"gender\" + 0.009*\"discuss\" + 0.008*\"posit\" + 0.007*\"commun\" + 0.007*\"help\" + 0.007*\"rape\"'),\n",
       " (4,\n",
       "  '0.023*\"friend\" + 0.020*\"life\" + 0.018*\"work\" + 0.014*\"help\" + 0.013*\"start\" + 0.008*\"school\" + 0.007*\"relationship\" + 0.007*\"look\" + 0.006*\"love\" + 0.006*\"problem\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3, increasing num_topics and passes\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.053*\"girl\" + 0.053*\"friend\" + 0.051*\"date\" + 0.025*\"relationship\" + 0.013*\"look\" + 0.012*\"girlfriend\" + 0.010*\"start\" + 0.009*\"text\" + 0.008*\"work\" + 0.007*\"meet\"'),\n",
       " (1,\n",
       "  '0.078*\"women\" + 0.031*\"woman\" + 0.013*\"masculin\" + 0.010*\"male\" + 0.010*\"gender\" + 0.008*\"gener\" + 0.008*\"issu\" + 0.007*\"discuss\" + 0.007*\"attract\" + 0.006*\"commun\"'),\n",
       " (2,\n",
       "  '0.022*\"help\" + 0.018*\"parent\" + 0.012*\"hair\" + 0.009*\"bodi\" + 0.007*\"money\" + 0.007*\"father\" + 0.007*\"mother\" + 0.007*\"famili\" + 0.006*\"deal\" + 0.006*\"need\"'),\n",
       " (3,\n",
       "  '0.029*\"life\" + 0.026*\"work\" + 0.013*\"relationship\" + 0.012*\"help\" + 0.010*\"love\" + 0.009*\"start\" + 0.009*\"school\" + 0.007*\"point\" + 0.007*\"chang\" + 0.007*\"care\"'),\n",
       " (4,\n",
       "  '0.053*\"delet\" + 0.014*\"masturb\" + 0.014*\"health\" + 0.012*\"help\" + 0.012*\"porn\" + 0.012*\"watch\" + 0.010*\"studi\" + 0.009*\"peni\" + 0.008*\"video\" + 0.007*\"research\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 4, increasing num_topics and passes\n",
    "ldan3 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15)\n",
    "ldan3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.039*\"women\" + 0.014*\"woman\" + 0.007*\"male\" + 0.007*\"delet\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.005*\"look\" + 0.005*\"bodi\" + 0.005*\"issu\" + 0.004*\"gender\"'),\n",
       " (1,\n",
       "  '0.021*\"friend\" + 0.020*\"girl\" + 0.020*\"date\" + 0.017*\"work\" + 0.016*\"relationship\" + 0.014*\"life\" + 0.010*\"help\" + 0.009*\"start\" + 0.008*\"look\" + 0.006*\"school\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 5\n",
    "ldan4 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.085*\"women\" + 0.057*\"date\" + 0.048*\"girl\" + 0.042*\"woman\" + 0.030*\"delet\" + 0.020*\"attract\" + 0.017*\"look\" + 0.012*\"partner\" + 0.010*\"bodi\" + 0.010*\"relationship\"'),\n",
       " (1,\n",
       "  '0.027*\"hair\" + 0.011*\"movi\" + 0.010*\"product\" + 0.009*\"studi\" + 0.007*\"style\" + 0.007*\"film\" + 0.007*\"watch\" + 0.007*\"work\" + 0.006*\"health\" + 0.006*\"routin\"'),\n",
       " (2,\n",
       "  '0.029*\"friend\" + 0.020*\"work\" + 0.020*\"relationship\" + 0.017*\"girl\" + 0.017*\"life\" + 0.014*\"date\" + 0.012*\"start\" + 0.009*\"help\" + 0.008*\"look\" + 0.007*\"school\"'),\n",
       " (3,\n",
       "  '0.021*\"parent\" + 0.021*\"help\" + 0.011*\"favorit\" + 0.010*\"famili\" + 0.009*\"money\" + 0.009*\"life\" + 0.009*\"need\" + 0.009*\"mother\" + 0.008*\"father\" + 0.008*\"wife\"'),\n",
       " (4,\n",
       "  '0.011*\"male\" + 0.011*\"masculin\" + 0.010*\"issu\" + 0.009*\"gender\" + 0.008*\"rule\" + 0.008*\"posit\" + 0.008*\"discuss\" + 0.008*\"commun\" + 0.007*\"gener\" + 0.006*\"help\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 6\n",
    "ldan5 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=5)\n",
    "ldan5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"women\" + 0.012*\"male\" + 0.012*\"masculin\" + 0.009*\"gender\" + 0.007*\"discuss\" + 0.007*\"issu\" + 0.007*\"gener\" + 0.006*\"view\" + 0.005*\"posit\" + 0.005*\"commun\"'),\n",
       " (1,\n",
       "  '0.043*\"friend\" + 0.031*\"girl\" + 0.024*\"relationship\" + 0.021*\"work\" + 0.019*\"date\" + 0.012*\"start\" + 0.010*\"girlfriend\" + 0.009*\"school\" + 0.008*\"tell\" + 0.007*\"home\"'),\n",
       " (2,\n",
       "  '0.033*\"life\" + 0.031*\"help\" + 0.017*\"work\" + 0.009*\"depress\" + 0.009*\"problem\" + 0.007*\"start\" + 0.007*\"need\" + 0.007*\"chang\" + 0.007*\"health\" + 0.007*\"thank\"'),\n",
       " (3,\n",
       "  '0.010*\"bodi\" + 0.008*\"look\" + 0.008*\"compliment\" + 0.007*\"woman\" + 0.007*\"ball\" + 0.006*\"hand\" + 0.006*\"creepi\" + 0.005*\"cloth\" + 0.005*\"receiv\" + 0.005*\"food\"'),\n",
       " (4,\n",
       "  '0.051*\"date\" + 0.027*\"woman\" + 0.025*\"girl\" + 0.025*\"delet\" + 0.021*\"look\" + 0.020*\"women\" + 0.017*\"attract\" + 0.016*\"relationship\" + 0.014*\"partner\" + 0.014*\"hair\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 7\n",
    "ldan6 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10)\n",
    "ldan6.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.089*\"women\" + 0.020*\"woman\" + 0.013*\"male\" + 0.010*\"gender\" + 0.008*\"discuss\" + 0.008*\"gener\" + 0.007*\"issu\" + 0.006*\"rape\" + 0.006*\"face\" + 0.006*\"view\"'),\n",
       " (1,\n",
       "  '0.060*\"girl\" + 0.058*\"date\" + 0.044*\"relationship\" + 0.019*\"friend\" + 0.017*\"look\" + 0.015*\"delet\" + 0.011*\"woman\" + 0.009*\"text\" + 0.009*\"girlfriend\" + 0.009*\"attract\"'),\n",
       " (2,\n",
       "  '0.035*\"friend\" + 0.027*\"work\" + 0.016*\"life\" + 0.015*\"school\" + 0.011*\"start\" + 0.009*\"parent\" + 0.009*\"colleg\" + 0.009*\"home\" + 0.006*\"tell\" + 0.006*\"doesnt\"'),\n",
       " (3,\n",
       "  '0.013*\"posit\" + 0.013*\"watch\" + 0.012*\"favorit\" + 0.011*\"masculin\" + 0.011*\"love\" + 0.010*\"movi\" + 0.007*\"play\" + 0.006*\"video\" + 0.006*\"film\" + 0.005*\"style\"'),\n",
       " (4,\n",
       "  '0.032*\"help\" + 0.019*\"life\" + 0.015*\"work\" + 0.010*\"depress\" + 0.009*\"problem\" + 0.009*\"look\" + 0.008*\"hair\" + 0.008*\"thank\" + 0.007*\"health\" + 0.007*\"need\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 8\n",
    "ldan7 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10)\n",
    "ldan7.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Attempt#3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pull out nuns and adjectives \n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>masculin need constantli insult averag mediocr insult caus masculin masculin masculin drive compet masculin masculin includ believ fear gender acknowledg respons forth motiv masculin desir foremos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date resourc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vunrabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mainstream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  all_text\n",
       "0  masculin need constantli insult averag mediocr insult caus masculin masculin masculin drive compet masculin masculin includ believ fear gender acknowledg respons forth motiv masculin desir foremos...\n",
       "1                                                                                                                                                                                             date resourc\n",
       "2                                                                                                                                                                                                  vunrabl\n",
       "3                                                                                                                                                                                                    women\n",
       "4                                                                                                                                                                                               mainstream"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns_adj = pd.DataFrame(mens_issues.all_text.apply(nouns_adj))\n",
    "#data_nouns_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwardmendoza/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['don'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaa</th>\n",
       "      <th>aaaaaaaaaahhhhhhhh</th>\n",
       "      <th>aaaaaaaaargh</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aawww</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonedi</th>\n",
       "      <th>abat</th>\n",
       "      <th>...</th>\n",
       "      <th>카카오스토리</th>\n",
       "      <th>카톡mad</th>\n",
       "      <th>키워드광고</th>\n",
       "      <th>페이스북</th>\n",
       "      <th>페이스북광고</th>\n",
       "      <th>홈페이지만드는법</th>\n",
       "      <th>홈페이지제작방법</th>\n",
       "      <th>홈페이지제작사이트</th>\n",
       "      <th>홈페이지제작업체</th>\n",
       "      <th>홈페이지형블로그</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaaa  aaaaaaaaaahhhhhhhh  aaaaaaaaargh  aardvark  aaron  aawww  aback  \\\n",
       "0        0                   0             0         0      0      0      0   \n",
       "1        0                   0             0         0      0      0      0   \n",
       "2        0                   0             0         0      0      0      0   \n",
       "3        0                   0             0         0      0      0      0   \n",
       "4        0                   0             0         0      0      0      0   \n",
       "\n",
       "   abandon  abandonedi  abat  ...  카카오스토리  카톡mad  키워드광고  페이스북  페이스북광고  \\\n",
       "0        0           0     0  ...       0      0      0     0       0   \n",
       "1        0           0     0  ...       0      0      0     0       0   \n",
       "2        0           0     0  ...       0      0      0     0       0   \n",
       "3        0           0     0  ...       0      0      0     0       0   \n",
       "4        0           0     0  ...       0      0      0     0       0   \n",
       "\n",
       "   홈페이지만드는법  홈페이지제작방법  홈페이지제작사이트  홈페이지제작업체  홈페이지형블로그  \n",
       "0         0         0          0         0         0  \n",
       "1         0         0          0         0         0  \n",
       "2         0         0          0         0         0  \n",
       "3         0         0          0         0         0  \n",
       "4         0         0          0         0         0  \n",
       "\n",
       "[5 rows x 20266 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "\n",
    "cvna = CountVectorizer(stop_words=stop_words.stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.all_text)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "#data_dtmna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"date\" + 0.022*\"girl\" + 0.019*\"friend\" + 0.018*\"relationship\" + 0.018*\"work\" + 0.016*\"life\" + 0.010*\"help\" + 0.008*\"start\" + 0.007*\"look\" + 0.007*\"school\"'),\n",
       " (1,\n",
       "  '0.041*\"women\" + 0.015*\"woman\" + 0.013*\"delet\" + 0.008*\"male\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.005*\"issu\" + 0.005*\"bodi\" + 0.004*\"gener\" + 0.004*\"gender\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.037*\"date\" + 0.037*\"girl\" + 0.030*\"work\" + 0.026*\"relationship\" + 0.024*\"friend\" + 0.014*\"life\" + 0.014*\"help\" + 0.013*\"school\" + 0.010*\"start\" + 0.009*\"look\"'),\n",
       " (1,\n",
       "  '0.019*\"women\" + 0.017*\"masculin\" + 0.017*\"male\" + 0.012*\"gender\" + 0.012*\"issu\" + 0.010*\"health\" + 0.009*\"discuss\" + 0.009*\"help\" + 0.008*\"support\" + 0.007*\"commun\"'),\n",
       " (2,\n",
       "  '0.023*\"life\" + 0.014*\"friend\" + 0.012*\"problem\" + 0.012*\"delet\" + 0.011*\"deal\" + 0.009*\"depress\" + 0.008*\"hate\" + 0.008*\"help\" + 0.008*\"relationship\" + 0.007*\"point\"'),\n",
       " (3,\n",
       "  '0.069*\"women\" + 0.035*\"woman\" + 0.014*\"hair\" + 0.013*\"bodi\" + 0.013*\"partner\" + 0.012*\"look\" + 0.012*\"attract\" + 0.010*\"watch\" + 0.008*\"dick\" + 0.007*\"face\"'),\n",
       " (4,\n",
       "  '0.037*\"parent\" + 0.017*\"wife\" + 0.015*\"favorit\" + 0.014*\"famili\" + 0.014*\"mother\" + 0.013*\"father\" + 0.013*\"money\" + 0.011*\"turn\" + 0.011*\"marri\" + 0.011*\"children\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass2- Let's try 5 topics, 20 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=20)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"favorit\" + 0.013*\"movi\" + 0.012*\"watch\" + 0.009*\"wife\" + 0.009*\"think\" + 0.009*\"love\" + 0.008*\"babi\" + 0.008*\"play\" + 0.008*\"film\" + 0.008*\"compliment\"'),\n",
       " (1,\n",
       "  '0.037*\"date\" + 0.036*\"girl\" + 0.034*\"friend\" + 0.032*\"relationship\" + 0.009*\"start\" + 0.009*\"look\" + 0.008*\"life\" + 0.007*\"kind\" + 0.007*\"work\" + 0.007*\"woman\"'),\n",
       " (2,\n",
       "  '0.073*\"women\" + 0.015*\"male\" + 0.014*\"masculin\" + 0.010*\"gender\" + 0.009*\"issu\" + 0.008*\"woman\" + 0.007*\"discuss\" + 0.007*\"posit\" + 0.006*\"gener\" + 0.006*\"group\"'),\n",
       " (3,\n",
       "  '0.022*\"hair\" + 0.022*\"bodi\" + 0.017*\"woman\" + 0.016*\"look\" + 0.012*\"dick\" + 0.011*\"masturb\" + 0.009*\"size\" + 0.009*\"face\" + 0.009*\"turn\" + 0.009*\"watch\"'),\n",
       " (4,\n",
       "  '0.031*\"work\" + 0.029*\"life\" + 0.025*\"help\" + 0.012*\"school\" + 0.011*\"delet\" + 0.010*\"parent\" + 0.010*\"depress\" + 0.008*\"health\" + 0.008*\"home\" + 0.007*\"start\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3 - Let's try 5 topics, 30 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=30)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dating Advice\n",
    "2. ?? \n",
    "3. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"delet\" + 0.019*\"hair\" + 0.017*\"bodi\" + 0.014*\"watch\" + 0.011*\"favorit\" + 0.010*\"dick\" + 0.010*\"look\" + 0.008*\"movi\" + 0.007*\"size\" + 0.007*\"porn\"'),\n",
       " (1,\n",
       "  '0.104*\"relationship\" + 0.043*\"friend\" + 0.024*\"partner\" + 0.018*\"girlfriend\" + 0.012*\"cheat\" + 0.012*\"deal\" + 0.011*\"boyfriend\" + 0.010*\"marri\" + 0.010*\"woman\" + 0.009*\"masturb\"'),\n",
       " (2,\n",
       "  '0.025*\"women\" + 0.014*\"male\" + 0.014*\"masculin\" + 0.010*\"issu\" + 0.009*\"gender\" + 0.006*\"posit\" + 0.006*\"discuss\" + 0.006*\"support\" + 0.006*\"gener\" + 0.006*\"help\"'),\n",
       " (3,\n",
       "  '0.116*\"date\" + 0.116*\"girl\" + 0.069*\"women\" + 0.033*\"woman\" + 0.019*\"attract\" + 0.018*\"look\" + 0.013*\"number\" + 0.011*\"approach\" + 0.010*\"advic\" + 0.009*\"delet\"'),\n",
       " (4,\n",
       "  '0.024*\"work\" + 0.023*\"life\" + 0.016*\"friend\" + 0.014*\"help\" + 0.011*\"start\" + 0.010*\"school\" + 0.007*\"problem\" + 0.007*\"point\" + 0.006*\"depress\" + 0.006*\"place\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3 - Let's try 5 topics, 40 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=40)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"school\" + 0.021*\"work\" + 0.019*\"parent\" + 0.016*\"colleg\" + 0.013*\"friend\" + 0.009*\"class\" + 0.009*\"home\" + 0.008*\"weekend\" + 0.008*\"wife\" + 0.008*\"famili\"'),\n",
       " (1,\n",
       "  '0.033*\"life\" + 0.026*\"work\" + 0.023*\"help\" + 0.009*\"depress\" + 0.009*\"problem\" + 0.009*\"start\" + 0.007*\"point\" + 0.007*\"hate\" + 0.007*\"chang\" + 0.006*\"care\"'),\n",
       " (2,\n",
       "  '0.053*\"date\" + 0.053*\"girl\" + 0.048*\"relationship\" + 0.042*\"friend\" + 0.010*\"partner\" + 0.009*\"look\" + 0.008*\"text\" + 0.008*\"girlfriend\" + 0.008*\"convers\" + 0.007*\"advic\"'),\n",
       " (3,\n",
       "  '0.017*\"women\" + 0.016*\"male\" + 0.016*\"masculin\" + 0.011*\"issu\" + 0.011*\"gender\" + 0.007*\"posit\" + 0.007*\"discuss\" + 0.007*\"group\" + 0.006*\"commun\" + 0.006*\"view\"'),\n",
       " (4,\n",
       "  '0.106*\"women\" + 0.055*\"woman\" + 0.035*\"delet\" + 0.020*\"hair\" + 0.014*\"bodi\" + 0.014*\"look\" + 0.011*\"attract\" + 0.010*\"masturb\" + 0.008*\"face\" + 0.007*\"size\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 4 - Let's try 5 topics, 50 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=50)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"help\" + 0.017*\"work\" + 0.013*\"life\" + 0.012*\"health\" + 0.007*\"issu\" + 0.006*\"chang\" + 0.005*\"father\" + 0.005*\"support\" + 0.005*\"parent\" + 0.005*\"school\"'),\n",
       " (1,\n",
       "  '0.019*\"hair\" + 0.019*\"bodi\" + 0.016*\"look\" + 0.011*\"dick\" + 0.009*\"masturb\" + 0.009*\"watch\" + 0.008*\"size\" + 0.007*\"face\" + 0.007*\"porn\" + 0.006*\"kind\"'),\n",
       " (2,\n",
       "  '0.070*\"women\" + 0.021*\"masculin\" + 0.021*\"male\" + 0.015*\"gender\" + 0.009*\"discuss\" + 0.009*\"favorit\" + 0.008*\"group\" + 0.008*\"posit\" + 0.008*\"toxic\" + 0.008*\"role\"'),\n",
       " (3,\n",
       "  '0.070*\"date\" + 0.069*\"girl\" + 0.054*\"relationship\" + 0.027*\"woman\" + 0.025*\"women\" + 0.018*\"delet\" + 0.013*\"partner\" + 0.011*\"attract\" + 0.009*\"number\" + 0.009*\"text\"'),\n",
       " (4,\n",
       "  '0.031*\"friend\" + 0.023*\"work\" + 0.023*\"life\" + 0.014*\"start\" + 0.012*\"help\" + 0.010*\"school\" + 0.008*\"problem\" + 0.008*\"home\" + 0.008*\"point\" + 0.007*\"deal\"')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 5 - Let's try 5 topics, 25 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=25)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.127*\"women\" + 0.053*\"woman\" + 0.045*\"date\" + 0.032*\"relationship\" + 0.023*\"partner\" + 0.020*\"attract\" + 0.010*\"boyfriend\" + 0.008*\"curiou\" + 0.008*\"marri\" + 0.008*\"consid\"'),\n",
       " (1,\n",
       "  '0.041*\"work\" + 0.024*\"help\" + 0.015*\"parent\" + 0.014*\"school\" + 0.014*\"life\" + 0.013*\"health\" + 0.010*\"home\" + 0.009*\"money\" + 0.008*\"colleg\" + 0.007*\"famili\"'),\n",
       " (2,\n",
       "  '0.053*\"delet\" + 0.031*\"hair\" + 0.029*\"bodi\" + 0.028*\"look\" + 0.017*\"dick\" + 0.015*\"girl\" + 0.014*\"favorit\" + 0.012*\"face\" + 0.011*\"compliment\" + 0.007*\"want\"'),\n",
       " (3,\n",
       "  '0.022*\"masturb\" + 0.020*\"virgin\" + 0.020*\"think\" + 0.019*\"rape\" + 0.016*\"peni\" + 0.016*\"porn\" + 0.014*\"watch\" + 0.011*\"consent\" + 0.011*\"size\" + 0.010*\"beer\"'),\n",
       " (4,\n",
       "  '0.019*\"masculin\" + 0.017*\"male\" + 0.013*\"gender\" + 0.012*\"issu\" + 0.009*\"posit\" + 0.008*\"group\" + 0.007*\"role\" + 0.007*\"commun\" + 0.007*\"toxic\" + 0.007*\"discuss\"'),\n",
       " (5,\n",
       "  '0.026*\"friend\" + 0.025*\"girl\" + 0.020*\"life\" + 0.017*\"relationship\" + 0.017*\"date\" + 0.010*\"start\" + 0.009*\"work\" + 0.009*\"help\" + 0.007*\"problem\" + 0.007*\"doesnt\"')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 6 - Let's try 6 topics, 50 passes\n",
    "ldana5 = models.LdaModel(corpus=corpusna, num_topics=6, id2word=id2wordna, passes=50)\n",
    "ldana5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.036*\"help\" + 0.021*\"issu\" + 0.018*\"problem\" + 0.016*\"emot\" + 0.013*\"health\" + 0.013*\"need\" + 0.011*\"relationship\" + 0.009*\"life\" + 0.008*\"think\" + 0.007*\"complet\"'),\n",
       " (1,\n",
       "  '0.041*\"women\" + 0.019*\"masculin\" + 0.016*\"male\" + 0.013*\"gender\" + 0.010*\"posit\" + 0.009*\"gener\" + 0.008*\"group\" + 0.008*\"discuss\" + 0.008*\"role\" + 0.007*\"toxic\"'),\n",
       " (2,\n",
       "  '0.043*\"work\" + 0.014*\"help\" + 0.013*\"watch\" + 0.009*\"favorit\" + 0.008*\"rape\" + 0.008*\"thank\" + 0.008*\"edit\" + 0.007*\"health\" + 0.007*\"product\" + 0.007*\"movi\"'),\n",
       " (3,\n",
       "  '0.045*\"friend\" + 0.015*\"girl\" + 0.014*\"start\" + 0.013*\"look\" + 0.010*\"kind\" + 0.008*\"doesnt\" + 0.008*\"work\" + 0.008*\"situat\" + 0.007*\"hair\" + 0.007*\"convers\"'),\n",
       " (4,\n",
       "  '0.103*\"delet\" + 0.041*\"bodi\" + 0.032*\"dick\" + 0.023*\"size\" + 0.019*\"peni\" + 0.019*\"curiou\" + 0.018*\"want\" + 0.014*\"face\" + 0.013*\"penis\" + 0.012*\"shower\"'),\n",
       " (5,\n",
       "  '0.043*\"life\" + 0.024*\"work\" + 0.018*\"school\" + 0.013*\"parent\" + 0.011*\"help\" + 0.010*\"depress\" + 0.010*\"colleg\" + 0.009*\"home\" + 0.009*\"famili\" + 0.008*\"hate\"'),\n",
       " (6,\n",
       "  '0.120*\"date\" + 0.084*\"girl\" + 0.077*\"relationship\" + 0.065*\"women\" + 0.040*\"woman\" + 0.019*\"attract\" + 0.016*\"partner\" + 0.015*\"girlfriend\" + 0.011*\"cheat\" + 0.009*\"advic\"')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 7 topics, 20 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=20)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"help\" + 0.039*\"life\" + 0.021*\"depress\" + 0.019*\"health\" + 0.016*\"work\" + 0.011*\"struggl\" + 0.010*\"anxieti\" + 0.008*\"problem\" + 0.008*\"need\" + 0.008*\"emot\"'),\n",
       " (1,\n",
       "  '0.085*\"delet\" + 0.029*\"boyfriend\" + 0.024*\"advic\" + 0.017*\"think\" + 0.011*\"thank\" + 0.011*\"particip\" + 0.010*\"studi\" + 0.010*\"research\" + 0.010*\"shower\" + 0.008*\"hung\"'),\n",
       " (2,\n",
       "  '0.078*\"women\" + 0.015*\"masculin\" + 0.014*\"male\" + 0.010*\"gender\" + 0.009*\"issu\" + 0.007*\"posit\" + 0.007*\"gener\" + 0.007*\"group\" + 0.007*\"discuss\" + 0.006*\"role\"'),\n",
       " (3,\n",
       "  '0.022*\"watch\" + 0.015*\"wife\" + 0.013*\"look\" + 0.013*\"favorit\" + 0.012*\"father\" + 0.011*\"mother\" + 0.011*\"video\" + 0.010*\"masturb\" + 0.009*\"marri\" + 0.009*\"play\"'),\n",
       " (4,\n",
       "  '0.074*\"date\" + 0.057*\"work\" + 0.022*\"school\" + 0.019*\"girl\" + 0.015*\"life\" + 0.015*\"colleg\" + 0.012*\"parent\" + 0.012*\"start\" + 0.011*\"home\" + 0.011*\"number\"'),\n",
       " (5,\n",
       "  '0.037*\"friend\" + 0.035*\"relationship\" + 0.029*\"girl\" + 0.012*\"life\" + 0.010*\"doesnt\" + 0.010*\"start\" + 0.009*\"deal\" + 0.009*\"kind\" + 0.008*\"love\" + 0.008*\"care\"'),\n",
       " (6,\n",
       "  '0.088*\"woman\" + 0.036*\"bodi\" + 0.036*\"hair\" + 0.024*\"attract\" + 0.020*\"dick\" + 0.015*\"partner\" + 0.014*\"size\" + 0.013*\"turn\" + 0.012*\"peni\" + 0.011*\"virgin\"')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 7 topics, 30 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=30)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"relationship\" + 0.030*\"life\" + 0.023*\"work\" + 0.022*\"help\" + 0.010*\"school\" + 0.009*\"parent\" + 0.008*\"depress\" + 0.007*\"chang\" + 0.007*\"problem\" + 0.006*\"colleg\"'),\n",
       " (1,\n",
       "  '0.042*\"delet\" + 0.013*\"favorit\" + 0.010*\"watch\" + 0.008*\"movi\" + 0.008*\"think\" + 0.007*\"valentin\" + 0.007*\"health\" + 0.006*\"film\" + 0.006*\"studi\" + 0.006*\"song\"'),\n",
       " (2,\n",
       "  '0.036*\"date\" + 0.036*\"girl\" + 0.033*\"friend\" + 0.012*\"work\" + 0.011*\"look\" + 0.010*\"start\" + 0.007*\"kind\" + 0.007*\"girlfriend\" + 0.006*\"text\" + 0.006*\"doesnt\"'),\n",
       " (3,\n",
       "  '0.066*\"women\" + 0.029*\"woman\" + 0.012*\"male\" + 0.010*\"masculin\" + 0.008*\"gender\" + 0.007*\"gener\" + 0.007*\"hair\" + 0.007*\"issu\" + 0.007*\"bodi\" + 0.007*\"partner\"')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5th Attempt, 20 passes, 4 topics\n",
    "ldana5 = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=20)\n",
    "ldana5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"friend\" + 0.023*\"life\" + 0.022*\"girl\" + 0.021*\"relationship\" + 0.017*\"work\" + 0.016*\"date\" + 0.010*\"start\" + 0.009*\"school\" + 0.009*\"help\" + 0.007*\"look\"'),\n",
       " (1,\n",
       "  '0.029*\"help\" + 0.023*\"work\" + 0.020*\"health\" + 0.007*\"support\" + 0.006*\"studi\" + 0.006*\"thank\" + 0.006*\"need\" + 0.005*\"watch\" + 0.005*\"money\" + 0.005*\"complet\"'),\n",
       " (2,\n",
       "  '0.034*\"delet\" + 0.022*\"bodi\" + 0.021*\"hair\" + 0.014*\"look\" + 0.011*\"dick\" + 0.010*\"masturb\" + 0.009*\"size\" + 0.008*\"face\" + 0.007*\"peni\" + 0.006*\"sport\"'),\n",
       " (3,\n",
       "  '0.019*\"women\" + 0.019*\"masculin\" + 0.016*\"male\" + 0.012*\"gender\" + 0.011*\"issu\" + 0.009*\"posit\" + 0.008*\"discuss\" + 0.008*\"group\" + 0.007*\"commun\" + 0.007*\"role\"'),\n",
       " (4,\n",
       "  '0.095*\"women\" + 0.051*\"woman\" + 0.032*\"date\" + 0.021*\"partner\" + 0.016*\"attract\" + 0.010*\"boyfriend\" + 0.010*\"favorit\" + 0.008*\"curiou\" + 0.007*\"movi\" + 0.006*\"marri\"')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6th Attempt, 80 passes, 5 topics\n",
    "ldana6 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80)\n",
    "ldana6.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"delet\" + 0.015*\"watch\" + 0.014*\"wife\" + 0.014*\"parent\" + 0.012*\"mother\" + 0.012*\"father\" + 0.008*\"bodi\" + 0.008*\"masturb\" + 0.008*\"money\" + 0.006*\"marri\"'),\n",
       " (1,\n",
       "  '0.045*\"women\" + 0.015*\"male\" + 0.010*\"gender\" + 0.009*\"issu\" + 0.006*\"posit\" + 0.006*\"group\" + 0.006*\"discuss\" + 0.006*\"commun\" + 0.006*\"gener\" + 0.006*\"role\"'),\n",
       " (2,\n",
       "  '0.045*\"date\" + 0.045*\"girl\" + 0.042*\"relationship\" + 0.028*\"friend\" + 0.018*\"woman\" + 0.014*\"women\" + 0.013*\"look\" + 0.008*\"partner\" + 0.008*\"kind\" + 0.008*\"girlfriend\"'),\n",
       " (3,\n",
       "  '0.045*\"masculin\" + 0.021*\"toxic\" + 0.016*\"favorit\" + 0.010*\"feminin\" + 0.009*\"song\" + 0.008*\"tran\" + 0.008*\"video\" + 0.007*\"product\" + 0.006*\"birthday\" + 0.006*\"kind\"'),\n",
       " (4,\n",
       "  '0.031*\"life\" + 0.031*\"work\" + 0.020*\"help\" + 0.013*\"school\" + 0.011*\"friend\" + 0.010*\"start\" + 0.009*\"depress\" + 0.008*\"problem\" + 0.007*\"home\" + 0.007*\"point\"')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7th Attempt, 90 passes, 5 topics\n",
    "ldana7 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=90)\n",
    "ldana7.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"bodi\" + 0.021*\"hair\" + 0.016*\"look\" + 0.015*\"watch\" + 0.012*\"favorit\" + 0.011*\"dick\" + 0.011*\"movi\" + 0.010*\"masturb\" + 0.009*\"size\" + 0.008*\"face\"'),\n",
       " (1,\n",
       "  '0.026*\"life\" + 0.025*\"work\" + 0.014*\"help\" + 0.011*\"school\" + 0.010*\"friend\" + 0.009*\"start\" + 0.008*\"deal\" + 0.007*\"problem\" + 0.007*\"depress\" + 0.007*\"care\"'),\n",
       " (2,\n",
       "  '0.033*\"delet\" + 0.030*\"help\" + 0.017*\"health\" + 0.013*\"rape\" + 0.010*\"victim\" + 0.009*\"assault\" + 0.009*\"need\" + 0.009*\"male\" + 0.009*\"studi\" + 0.008*\"support\"'),\n",
       " (3,\n",
       "  '0.062*\"women\" + 0.016*\"masculin\" + 0.012*\"male\" + 0.011*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"woman\" + 0.007*\"group\" + 0.007*\"gener\" + 0.007*\"discuss\"'),\n",
       " (4,\n",
       "  '0.064*\"date\" + 0.063*\"girl\" + 0.048*\"relationship\" + 0.038*\"friend\" + 0.019*\"woman\" + 0.011*\"partner\" + 0.011*\"attract\" + 0.010*\"girlfriend\" + 0.010*\"text\" + 0.009*\"look\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8th Attempt, 100 passes, 5 topics\n",
    "ldana8 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=100)\n",
    "ldana8.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Topic 1 - > Body Insecurity\n",
    "##topic 2 - > General Issues\n",
    "##Topic 3 - > Sexual Assault\n",
    "##Topic 4 - > Gender roles/Male Masculinity\n",
    "##Topic 5 - > Dating and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating another model to add more stop words that don't help with honing down topics \"song\", \"life\" (too general),\n",
    "##\"look\", \"discuss\", and \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stop_words = ['song', 'life', 'look', 'discuss', 'text']\n",
    "stop_words2 = stop_words.stop_words + add_stop_words\n",
    "\n",
    "cvna2 = CountVectorizer(stop_words=stop_words2, max_df=.8)\n",
    "data_cvna2 = cvna2.fit_transform(data_nouns_adj.all_text)\n",
    "data_dtmna2 = pd.DataFrame(data_cvna2.toarray(), columns=cvna2.get_feature_names())\n",
    "data_dtmna2.index = data_nouns_adj.index\n",
    "#data_dtmna2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna2 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna2.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna2 = dict((v, k) for k, v in cvna2.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.057*\"delet\" + 0.032*\"hair\" + 0.021*\"watch\" + 0.020*\"favorit\" + 0.014*\"movi\" + 0.011*\"video\" + 0.009*\"film\" + 0.008*\"want\" + 0.007*\"prefer\" + 0.007*\"product\"'),\n",
       " (1,\n",
       "  '0.015*\"masculin\" + 0.015*\"issu\" + 0.012*\"health\" + 0.009*\"support\" + 0.009*\"posit\" + 0.008*\"help\" + 0.008*\"commun\" + 0.007*\"male\" + 0.006*\"gender\" + 0.006*\"toxic\"'),\n",
       " (2,\n",
       "  '0.095*\"women\" + 0.042*\"woman\" + 0.022*\"date\" + 0.016*\"attract\" + 0.015*\"partner\" + 0.011*\"male\" + 0.010*\"gener\" + 0.008*\"kind\" + 0.008*\"dick\" + 0.006*\"curiou\"'),\n",
       " (3,\n",
       "  '0.039*\"girl\" + 0.035*\"work\" + 0.031*\"date\" + 0.031*\"friend\" + 0.014*\"school\" + 0.013*\"help\" + 0.010*\"start\" + 0.008*\"home\" + 0.008*\"colleg\" + 0.006*\"parent\"'),\n",
       " (4,\n",
       "  '0.037*\"relationship\" + 0.010*\"help\" + 0.009*\"deal\" + 0.009*\"problem\" + 0.009*\"depress\" + 0.008*\"doesnt\" + 0.008*\"care\" + 0.007*\"love\" + 0.007*\"hate\" + 0.007*\"start\"')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 1\n",
    "ldana_1 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=20)\n",
    "ldana_1.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*\"help\" + 0.012*\"problem\" + 0.010*\"work\" + 0.009*\"school\" + 0.008*\"depress\" + 0.008*\"point\" + 0.007*\"hate\" + 0.006*\"issu\" + 0.006*\"care\" + 0.006*\"start\"'),\n",
       " (1,\n",
       "  '0.040*\"friend\" + 0.039*\"relationship\" + 0.030*\"work\" + 0.022*\"date\" + 0.018*\"girl\" + 0.012*\"start\" + 0.008*\"home\" + 0.008*\"colleg\" + 0.007*\"girlfriend\" + 0.007*\"love\"'),\n",
       " (2,\n",
       "  '0.029*\"masculin\" + 0.021*\"hair\" + 0.019*\"watch\" + 0.014*\"favorit\" + 0.013*\"posit\" + 0.011*\"movi\" + 0.010*\"role\" + 0.009*\"video\" + 0.008*\"porn\" + 0.008*\"male\"'),\n",
       " (3,\n",
       "  '0.013*\"gender\" + 0.012*\"male\" + 0.010*\"health\" + 0.010*\"support\" + 0.010*\"issu\" + 0.009*\"group\" + 0.009*\"rape\" + 0.007*\"children\" + 0.007*\"topic\" + 0.006*\"commun\"'),\n",
       " (4,\n",
       "  '0.084*\"women\" + 0.053*\"girl\" + 0.046*\"date\" + 0.036*\"woman\" + 0.023*\"delet\" + 0.014*\"attract\" + 0.010*\"bodi\" + 0.009*\"partner\" + 0.008*\"number\" + 0.007*\"approach\"')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 2\n",
    "ldana_2 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=30)\n",
    "ldana_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.061*\"woman\" + 0.015*\"father\" + 0.014*\"partner\" + 0.013*\"rape\" + 0.012*\"mother\" + 0.011*\"abus\" + 0.011*\"wife\" + 0.011*\"marri\" + 0.011*\"children\" + 0.009*\"victim\"'),\n",
       " (1,\n",
       "  '0.056*\"date\" + 0.049*\"relationship\" + 0.047*\"girl\" + 0.018*\"help\" + 0.017*\"friend\" + 0.010*\"girlfriend\" + 0.009*\"deal\" + 0.009*\"love\" + 0.008*\"depress\" + 0.007*\"start\"'),\n",
       " (2,\n",
       "  '0.058*\"women\" + 0.012*\"masculin\" + 0.011*\"male\" + 0.008*\"gender\" + 0.008*\"issu\" + 0.007*\"help\" + 0.007*\"posit\" + 0.006*\"gener\" + 0.006*\"commun\" + 0.005*\"health\"'),\n",
       " (3,\n",
       "  '0.029*\"work\" + 0.022*\"friend\" + 0.013*\"school\" + 0.010*\"start\" + 0.008*\"help\" + 0.008*\"point\" + 0.008*\"parent\" + 0.008*\"home\" + 0.008*\"place\" + 0.007*\"doesnt\"'),\n",
       " (4,\n",
       "  '0.042*\"delet\" + 0.024*\"bodi\" + 0.013*\"dick\" + 0.013*\"favorit\" + 0.012*\"masturb\" + 0.010*\"size\" + 0.008*\"peni\" + 0.007*\"face\" + 0.006*\"valentin\" + 0.006*\"kind\"')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 3\n",
    "ldana_3 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=40)\n",
    "ldana_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"work\" + 0.019*\"help\" + 0.011*\"school\" + 0.010*\"start\" + 0.008*\"depress\" + 0.008*\"parent\" + 0.008*\"home\" + 0.007*\"care\" + 0.007*\"problem\" + 0.006*\"point\"'),\n",
       " (1,\n",
       "  '0.060*\"women\" + 0.033*\"delet\" + 0.026*\"woman\" + 0.020*\"hair\" + 0.017*\"partner\" + 0.012*\"bodi\" + 0.011*\"rape\" + 0.009*\"masturb\" + 0.008*\"face\" + 0.007*\"assault\"'),\n",
       " (2,\n",
       "  '0.042*\"women\" + 0.016*\"masculin\" + 0.013*\"male\" + 0.011*\"issu\" + 0.011*\"gender\" + 0.007*\"view\" + 0.007*\"gener\" + 0.007*\"group\" + 0.007*\"posit\" + 0.006*\"role\"'),\n",
       " (3,\n",
       "  '0.057*\"date\" + 0.056*\"girl\" + 0.051*\"relationship\" + 0.046*\"friend\" + 0.010*\"girlfriend\" + 0.009*\"woman\" + 0.007*\"start\" + 0.007*\"kind\" + 0.007*\"convers\" + 0.007*\"attract\"'),\n",
       " (4,\n",
       "  '0.012*\"help\" + 0.010*\"health\" + 0.009*\"work\" + 0.008*\"thank\" + 0.008*\"studi\" + 0.007*\"product\" + 0.007*\"birthday\" + 0.006*\"research\" + 0.006*\"list\" + 0.006*\"particip\"')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 4\n",
    "ldana_4 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=50)\n",
    "ldana_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"partner\" + 0.012*\"health\" + 0.010*\"help\" + 0.010*\"support\" + 0.010*\"issu\" + 0.010*\"male\" + 0.009*\"rape\" + 0.007*\"victim\" + 0.006*\"assault\" + 0.006*\"research\"'),\n",
       " (1,\n",
       "  '0.082*\"women\" + 0.034*\"woman\" + 0.018*\"masculin\" + 0.013*\"date\" + 0.013*\"attract\" + 0.012*\"gender\" + 0.012*\"male\" + 0.008*\"gener\" + 0.007*\"toxic\" + 0.006*\"group\"'),\n",
       " (2,\n",
       "  '0.025*\"delet\" + 0.019*\"girlfriend\" + 0.012*\"wife\" + 0.010*\"mother\" + 0.010*\"watch\" + 0.010*\"father\" + 0.009*\"marri\" + 0.009*\"parent\" + 0.009*\"brother\" + 0.008*\"cheat\"'),\n",
       " (3,\n",
       "  '0.024*\"friend\" + 0.023*\"work\" + 0.022*\"girl\" + 0.022*\"relationship\" + 0.020*\"date\" + 0.015*\"help\" + 0.011*\"start\" + 0.010*\"school\" + 0.007*\"problem\" + 0.007*\"point\"'),\n",
       " (4,\n",
       "  '0.019*\"bodi\" + 0.018*\"hair\" + 0.010*\"dick\" + 0.008*\"watch\" + 0.008*\"love\" + 0.007*\"favorit\" + 0.007*\"size\" + 0.007*\"face\" + 0.007*\"posit\" + 0.006*\"peni\"')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 5\n",
    "#ldana_5 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=60)\n",
    "#ldana_5.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Topic 1 --> Sexual Trauma\\\n",
    "##Topic 2--> Gender Identity/Roles\\\n",
    "##Topic 3--> Familial Issues\\\n",
    "##topic 4--> Dating Advice/Relationships\\\n",
    "##Topic 5--> Body Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

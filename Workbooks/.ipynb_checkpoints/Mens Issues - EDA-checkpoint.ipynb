{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported the Following Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import stop_words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, plot_roc_curve, roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bro_pill = pd.read_csv('./bro_pill.csv')\n",
    "male_mental_health = pd.read_csv('./malementalhealth.csv')\n",
    "mens_lib = pd.read_csv('./Mens_Lib.csv')\n",
    "ask_men = pd.read_csv('./ask_men.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dfs = [mens_lib, male_mental_health, bro_pill, ask_men]\n",
    "\n",
    "for df in combined_dfs:\n",
    "    df.columns = ['title', 'selftext', 'subreddit', 'created_utc',\n",
    "                  'author', 'num_comments', 'score', 'is_self', 'timestamp']\n",
    "\n",
    "pd.concat(combined_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues = pd.concat(combined_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues = mens_issues.drop(columns = ['created_utc', 'num_comments', 'score', 'is_self', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mens_issues.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing autobots from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues = mens_issues[(mens_issues['author'] != 'MLModBot')]\n",
    "mens_issues = mens_issues[(mens_issues['author'] != 'Mr_Holmes')]\n",
    "mens_issues = mens_issues[(mens_issues['author'] != 'Dewey_Darl')] \n",
    "mens_issues = mens_issues[(mens_issues['author'] != 'AutoModerator')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining title and self text to ensure better text quality of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mens_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing `NaN` with an empty string so as concatenating the title and author will perform correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['selftext'].fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove string, specifically the [removed] string under the selftext column\n",
    "def remove_string(df, column, string):\n",
    "    df[column] = df[column].str.replace(string, '')\n",
    "\n",
    "remove_string(mens_issues, 'selftext', '\\[removed\\]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = mens_issues['title'].str.cat(mens_issues['selftext'], sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will create a function to normalize and standardize the text within the subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credits to Gwen for function\n",
    "def clean_strings(input_list, stopwords = []):\n",
    "    import re # we'll use regex to strip urls\n",
    "    output_list = [] # create output list\n",
    "    stopwords = [word.lower() for word in stopwords] # ensure case insensitivity for stopwords\n",
    "    for sentence in input_list:\n",
    "        sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(sentence), flags=re.MULTILINE) # remove URLS per stackoverflow\n",
    "        sentence = sentence.replace('\\n',' ') # replace \\n and \\t with spaces as they represent breaks between words\n",
    "        sentence = sentence.replace('\\t',' ')\n",
    "        sentence = ''.join([letter for letter in sentence if letter.isalpha() or letter == ' ']) #remove numbers and punctuation\n",
    "        sentence = ' '.join([word for word in sentence.split() if len(word) < 25]) #exclude egregiously long words\n",
    "        sentence = ' '.join([word.lower() for word in sentence.split() if word.lower() not in stopwords]) #coerce to lowercase while removing stopwords\n",
    "        sentence = re.sub(r'\\b\\w{1,3}\\b', '', str(sentence)) #removing words that are less than 3 letters long\n",
    "        sentence = re.sub('[‘’“”…]', '', sentence)\n",
    "        sentence = re.sub('\\n', '', sentence)\n",
    "        output_list.append(sentence) # add to the output list\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding stopwords that I deem to be insufficient for my analysis\n",
    "mens_issues['all_text'] = clean_strings(mens_issues['all_text'], stopwords = stop_words.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for nulls\n",
    "mens_issues.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created function to lemmatize each word in all text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(txt):\n",
    "    \"\"\"lemmatize words accepts a string\"\"\"\n",
    "    words = txt.split()\n",
    "    lem_words = ''\n",
    "    for word in words:\n",
    "        lem_words += (lemmatizer.lemmatize(word) + ' ')\n",
    "    return lem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving this version for topic modeling in the future\n",
    "#mens_issues.to_csv('./men_df_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created function to stemmatize each word in all text\n",
    "stemmatizer = PorterStemmer()\n",
    "\n",
    "def stemmatize_words(txt):\n",
    "    \"\"\"stemmatize words accepts a string\"\"\"\n",
    "    words = txt.split()\n",
    "    stem_words = ''\n",
    "    for word in words:\n",
    "        stem_words += (stemmatizer.stem(word) + ' ')\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = mens_issues['all_text'].apply(stemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving this version for topic modeling in the future\n",
    "#mens_issues.to_csv('./men_df_stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis on Mens' Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating the variety of string length\n",
    "mens_issues['post_length'] = mens_issues['all_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.displot(mens_issues, x=\"post_length\", legend=False, element=\"step\", bins = 500)\n",
    "plt.xlim(-1,2_000)\n",
    "plt.title(\"Post Length Distribution per Subreddit\\nexcluding outliers (>500)\")\n",
    "plt.show(g)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see majority of the posts fall within the 0-100 words bucket, yet we can't ignore the posts that exceed the majority amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_eda = CountVectorizer(stop_words = 'english', max_features = 20_000)\n",
    "\n",
    "X_eda = mens_issues['all_text']\n",
    "X_eda = cvec_eda.fit_transform(X_eda)\n",
    "X_eda_df = pd.DataFrame(X_eda.todense(), columns = cvec_eda.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eda_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eda_df.sum().sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "X_eda_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');\n",
    "plt.savefig('eda_words.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Most Common Words with Varying String Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating function to filter out words that are less than 4 letters long\n",
    "def clean_strings2(input_list, stopwords = []):\n",
    "    import re # we'll use regex to strip urls\n",
    "    output_list = [] # create output list\n",
    "    stopwords = [word.lower() for word in stopwords] # ensure case insensitivity for stopwords\n",
    "    for sentence in input_list:\n",
    "        sentence = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(sentence), flags=re.MULTILINE) # remove URLS per stackoverflow\n",
    "        sentence = sentence.replace('\\n',' ') # replace \\n and \\t with spaces as they represent breaks between words\n",
    "        sentence = sentence.replace('\\t',' ')\n",
    "        sentence = ''.join([letter for letter in sentence if letter.isalpha() or letter == ' ']) #remove numbers and punctuation\n",
    "        sentence = ' '.join([word for word in sentence.split() if len(word) > 4])\n",
    "        sentence = ' '.join([word.lower() for word in sentence.split() if word.lower() not in stopwords]) #coerce to lowercase while removing stopwords\n",
    "        sentence = re.sub(r'\\b\\w{1,3}\\b', '', str(sentence)) #removing words that are less than 3 letters long\n",
    "        sentence = re.sub('[‘’“”…]', '', sentence)\n",
    "        sentence = re.sub('\\n', '', sentence)\n",
    "        output_list.append(sentence) # add to the output list\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding stopwords that I deem to be insufficient for my analysis\n",
    "mens_issues['all_text2'] = clean_strings2(mens_issues['all_text'], stopwords = stop_words.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_eda2 = CountVectorizer(stop_words = 'english', max_features = 10_000)\n",
    "\n",
    "X_eda = mens_issues['all_text2']\n",
    "X_eda = cvec_eda2.fit_transform(X_eda)\n",
    "X_eda_df = pd.DataFrame(X_eda.todense(), columns = cvec_eda2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eda_df.sum().sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "X_eda_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');\n",
    "#plt.savefig('eda_words2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing corpus variable to list out the mens_issues['all_text'] column\n",
    "corpus = list(mens_issues['all_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self-Checking polarity scores for the 10th document\n",
    "sia.polarity_scores(corpus[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []    \n",
    "\n",
    "for el in corpus:\n",
    "    scores = sia.polarity_scores(el)\n",
    "    scores['alltext'] = el\n",
    "    sentiment.append(scores)\n",
    "\n",
    "df_sentiment = pd.DataFrame(sentiment)\n",
    "df_sentiment.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing marker column \n",
    "df_sentiment['score'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the average positive and negative scores for all posts\n",
    "df_sentiment.groupby('score').mean()[['pos', 'neg']].plot(kind='barh');\n",
    "plt.title('Average Positive, Negative & Compound Scores');\n",
    "plt.ylabel('All Posts');\n",
    "plt.xlabel('Sentiment Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigating posts with high negative sentiment scores\n",
    "df_sentiment[['neg', 'pos', 'alltext']].sort_values('neg', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigating posts with high sentiment scores\n",
    "df_sentiment[['neg', 'pos', 'alltext']].sort_values('pos', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By investigating the posts with high negative and positive sentiment scores, we see that most posts are solely one-worded posts. Will filter these posts out to get a clearer idea of what most men are posting on these subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created column where it returns either True or False whether or not the string length is more than 50 characters long\n",
    "df_sentiment['alltext_length'] = df_sentiment['alltext'].str.len() >= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created another dataframe where string length is more than 50\n",
    "second_df_sentiment = df_sentiment[df_sentiment['alltext_length'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi_df = second_df_sentiment[['neg', 'pos', 'alltext']].sort_values('pos', ascending=False).head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_eda_pos = CountVectorizer(stop_words = 'english', max_features = 20_000)\n",
    "\n",
    "posi_eda = posi_df['alltext']\n",
    "posi_eda = cvec_eda_pos.fit_transform(posi_eda)\n",
    "posi_eda_df = pd.DataFrame(posi_eda.todense(), columns = cvec_eda_pos.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self-checking values\n",
    "posi_eda_df.sum().sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "posi_eda_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');\n",
    "plt.savefig('posi_eda_words.png', dpi=200)\n",
    "plt.show();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_df_sentiment[['neg', 'pos', 'alltext']].sort_values('neg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = second_df_sentiment[['neg', 'pos', 'alltext']].sort_values('neg', ascending=False).head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_eda_neg = CountVectorizer(stop_words = 'english', max_features = 20_000)\n",
    "\n",
    "neg_eda = neg_df['alltext']\n",
    "neg_eda = cvec_eda_neg.fit_transform(neg_eda)\n",
    "neg_eda_df = pd.DataFrame(neg_eda.todense(), columns = cvec_eda_neg.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_eda_df.sum().sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7), dpi=100)\n",
    "neg_eda_df.sum().sort_values(ascending=False).head(15).plot(kind='barh');\n",
    "plt.savefig('neg_eda_words.png', dpi=200)\n",
    "plt.show();\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the EDA, I will explore the top bi-grams and tri-grams within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "men_say= mens_issues['all_text'].str.split()\n",
    "men_say=mens_issues.values.tolist()\n",
    "corpus=[word for i in men_say for word in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = clean_strings(mens_issues['all_text'], stopwords = stop_words.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import  Counter\n",
    "\n",
    "def plot_top_ngrams_barchart(text, n=2):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    new_stopwords=['have', 'been', 'this', 'that', 'have', 'have been', 'they were', 'away from', 'they', 'rules still apply', 'even', 'though', 'even though', 'amazon gift card']\n",
    "    new_stopwords_list = stop_words.union(new_stopwords)\n",
    "\n",
    "    men_say= mens_issues['all_text'].str.split()\n",
    "    men_say=mens_issues.values.tolist()\n",
    "    corpus=[word for i in men_say for word in i if word not in new_stopwords_list]\n",
    "\n",
    "    def _get_top_ngram(corpus, n=None):\n",
    "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) \n",
    "                      for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:10]\n",
    "\n",
    "    top_n_bigrams=_get_top_ngram(text,n)[:10]\n",
    "    x,y=map(list,zip(*top_n_bigrams))\n",
    "    sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "    sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_top_ngrams_barchart(mens_issues['all_text'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the majority of words appear to be referring to so something or someone. What really stood out here is \"high school\" being a top bi-gram. Furthermore, it appears that sexual assault, social media, and toxic masculinity are top words being represented in these sub reddit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_top_ngrams_barchart(mens_issues['all_text'],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.to_csv('./men_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = mens_issues.index\n",
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tdm = mens_issues.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm.drop(['title', 'selftext'], axis=0, inplace=True)\n",
    "tdm.drop(['subreddit', 'author'], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "add_stop_words = ['im', 'just', 'like', \"don\\'t\", 'feel', 'men', 'people', 'time', 'dont', 'make', 'know', 'ive', 'really', 'removed', 'nan', 'got', 'did', 'lot', 'have', 'been']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = mens_issues.index\n",
    "\n",
    "# Pickle it for later use\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda2 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=5)\n",
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=5)\n",
    "lda3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these passes of topic modeling via LDA, it's difficult to interpret the topics that we have here. Will adjust the words that will be passed through the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits to Alice Zhao\n",
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = mens_issues.all_text.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['im', 'just', 'like', \"don\\'t\", 'feel', 'men', 'people', 'time', 'dont', 'make', 'know', 'ive', 'really', 'removed', 'nan', 'got', 'did', 'lot', 'have', 'been', 'askmen', 'reddit']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(mens_issues.all_text)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "#data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on first pass, words like \"askmen\" and \"reddit\" shouldn't be a result for topic modeling, will add that to the stop words list.\\\n",
    "Overall, much more cogent words are being produced, but nothing really stands out where I can definitively say that the result being produced is a particular topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 2, excluding words of askmen and reddit\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pass on the model seems more promising. It appears that the first topic is in regards of social relationships where as the second second topic appears to be about the physical features of men (hair, body, masculinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 3, increasing num_topics and passes\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It appears that the same words are being reflected \"years\", \"year\", \"women\", \"woman\". Will Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pass 4, increasing num_topics and passes\n",
    "ldan3 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15)\n",
    "ldan3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems like I still gotta few more words that I need to add into the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass 5\n",
    "ldan4 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks promising, will increase num_topics to 5, 7 and 10 and see if anything substantial shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass 6\n",
    "ldan5 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=5)\n",
    "ldan5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass 7\n",
    "ldan6 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10)\n",
    "ldan6.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass 8 (with stemmatizer)\n",
    "ldan7 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10)\n",
    "ldan7.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Attempt#3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pull out nuns and adjectives \n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_nouns_adj = pd.DataFrame(mens_issues.all_text.apply(nouns_adj))\n",
    "data_nouns_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "\n",
    "\n",
    "add_stop_words = ['im', 'just', 'like', \"don\\'t\", 'feel', 'men', 'people', 'time', 'dont', 'make', 'know', 'ive', 'really', 'removed', 'nan', 'got', 'did', 'lot', 'have', 'been', 'askmen', 'reddit', 'whats', 'question', 'thing', 'things', 'thats', 'that', 'shes', 'year', 'years', 'youv', 'because', 'talk', 'month', 'post', 'anyone', 'anyon']\n",
    "stop_words_revised = add_stop_words + stop_words\n",
    "\n",
    "cvna = CountVectorizer(stop_words=stop_words_revised, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.all_text)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pass2- Let's try 5 topics, 20 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=20)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pass 3 - Let's try 5 topics, 30 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=30)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dating Advice\n",
    "2. ?? \n",
    "3. ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pass 3 - Let's try 5 topics, 40 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=40)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 4 - Let's try 5 topics, 50 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=50)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic 1 --> Sexual Assault/Trauma\n",
    "#Topic 2 --> Stuck in a rut\n",
    "#Topic 3 --> Body insecurities\n",
    "#Topic 4 --> Relationship Advice\n",
    "#Topic 5 --> Gender Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 5 - Let's try 5 topics, 25 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=25)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 6 - Let's try 6 topics, 50 passes\n",
    "ldana5 = models.LdaModel(corpus=corpusna, num_topics=6, id2word=id2wordna, passes=50)\n",
    "ldana5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass 7 - Let's try 5 topics, 80 passes\n",
    "ldana7 = models.LdaModel(corpus=corpusna, num_topics=6, id2word=id2wordna, passes=80)\n",
    "ldana7.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try 7 topics, 20 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=20)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 7 topics, 30 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=30)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words like: Years, shes, didnt don't contribute much to my analysis, will be adding them to the stopwords list and attempt again with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try 6 topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

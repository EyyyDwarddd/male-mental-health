{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported the Following Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/edwardmendoza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/edwardmendoza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import stop_words\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added a second python notebook due to realizing that a random_state is needed to ensure results are the same everytime a model is run.\n",
    "\n",
    "Apologies in advance, but please use this notebook to run the models accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues = pd.read_csv('../Datasets/men_df_eda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mens_issues.drop(columns = ['Unnamed: 0', 'title', 'selftext', 'subreddit', 'author', 'post_length', 'all_text2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date resourc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vunrabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walk away  women abort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mainstream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                  all_text\n",
       "0  masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...\n",
       "1                                                                                                                                                                                             date resourc\n",
       "2                                                                                                                                                                                                  vunrabl\n",
       "3                                                                                                                                                                                   walk away  women abort\n",
       "4                                                                                                                                                                                               mainstream"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mens_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mens_issues.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26595</th>\n",
       "      <th>26596</th>\n",
       "      <th>26597</th>\n",
       "      <th>26598</th>\n",
       "      <th>26599</th>\n",
       "      <th>26600</th>\n",
       "      <th>26601</th>\n",
       "      <th>26602</th>\n",
       "      <th>26603</th>\n",
       "      <th>26604</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_text</th>\n",
       "      <td>masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...</td>\n",
       "      <td>date resourc</td>\n",
       "      <td>vunrabl</td>\n",
       "      <td>walk away  women abort</td>\n",
       "      <td>mainstream</td>\n",
       "      <td>think  popular discours harm convers better</td>\n",
       "      <td>stop abort</td>\n",
       "      <td>stereotyp</td>\n",
       "      <td>anybodi check palgrav handbook male psycholog mental health thought read good sourc mental health hard shell euro imagin  check thought</td>\n",
       "      <td>group better help woman wife  girl licens profession counselor start youtub channel address mental health free laugh return read therapist help deepli hurt silent struggl love work strong pull rea...</td>\n",
       "      <td>...</td>\n",
       "      <td>grow multipl sister  fightget ampnbsp look ampnbsp think effect ampnbsp effemin result ampnbsp present relationship</td>\n",
       "      <td>want  help boyfriend learn respons boyfriend relationship deterior graduat univers finish spring lucki great program work coupl cours finish graduat summer  work field work labour laid issu doesnt...</td>\n",
       "      <td>enjoy oral girl</td>\n",
       "      <td>male equival flower  girl flower nice gestur girl</td>\n",
       "      <td>decis life turn unpleas success</td>\n",
       "      <td>romant intim andor sexual relationship honestli think chang</td>\n",
       "      <td>arrang date bisexu continu annoy girl simpli practic process arrang date imposs lose continu string girl repli text vagu unhelp answer moment girl  date went text follow shed great want meet chat ...</td>\n",
       "      <td>favorit podcast listen</td>\n",
       "      <td>respond hand written letter express silli simpl straightforward edit decid good clariti thank honesti</td>\n",
       "      <td>grow mustach clip hair center leav grow contribut outer  mustach look mustach advic sure vari depend style sure worthwhil topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 26325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                            0      \\\n",
       "all_text  masculin fear ordinari need superior chosen endeavour constantli strive good better insult describ averag ordinari mediocr insult caus masculin threaten discuss particular aspect masculin aspect m...   \n",
       "\n",
       "                 1        2                        3           4      \\\n",
       "all_text  date resourc  vunrabl   walk away  women abort  mainstream   \n",
       "\n",
       "                                                5            6          7      \\\n",
       "all_text  think  popular discours harm convers better   stop abort  stereotyp   \n",
       "\n",
       "                                                                                                                                            8      \\\n",
       "all_text  anybodi check palgrav handbook male psycholog mental health thought read good sourc mental health hard shell euro imagin  check thought   \n",
       "\n",
       "                                                                                                                                                                                                            9      \\\n",
       "all_text  group better help woman wife  girl licens profession counselor start youtub channel address mental health free laugh return read therapist help deepli hurt silent struggl love work strong pull rea...   \n",
       "\n",
       "          ...  \\\n",
       "all_text  ...   \n",
       "\n",
       "                                                                                                                        26595  \\\n",
       "all_text  grow multipl sister  fightget ampnbsp look ampnbsp think effect ampnbsp effemin result ampnbsp present relationship   \n",
       "\n",
       "                                                                                                                                                                                                            26596  \\\n",
       "all_text  want  help boyfriend learn respons boyfriend relationship deterior graduat univers finish spring lucki great program work coupl cours finish graduat summer  work field work labour laid issu doesnt...   \n",
       "\n",
       "                     26597                                              26598  \\\n",
       "all_text   enjoy oral girl  male equival flower  girl flower nice gestur girl   \n",
       "\n",
       "                                      26599  \\\n",
       "all_text    decis life turn unpleas success   \n",
       "\n",
       "                                                                 26600  \\\n",
       "all_text   romant intim andor sexual relationship honestli think chang   \n",
       "\n",
       "                                                                                                                                                                                                            26601  \\\n",
       "all_text  arrang date bisexu continu annoy girl simpli practic process arrang date imposs lose continu string girl repli text vagu unhelp answer moment girl  date went text follow shed great want meet chat ...   \n",
       "\n",
       "                           26602  \\\n",
       "all_text  favorit podcast listen   \n",
       "\n",
       "                                                                                                          26603  \\\n",
       "all_text  respond hand written letter express silli simpl straightforward edit decid good clariti thank honesti   \n",
       "\n",
       "                                                                                                                                    26604  \n",
       "all_text  grow mustach clip hair center leav grow contribut outer  mustach look mustach advic sure vari depend style sure worthwhil topic  \n",
       "\n",
       "[1 rows x 26325 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm = mens_issues.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaaaa</th>\n",
       "      <th>aaaaaaaaaahhhhhhhh</th>\n",
       "      <th>aaaaaaaaargh</th>\n",
       "      <th>aaaaaaaanyway</th>\n",
       "      <th>aardvark</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aawww</th>\n",
       "      <th>ababa</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>ì¹´ì¹´ì¤ì¤í ë¦¬</th>\n",
       "      <th>ì¹´í¡mad</th>\n",
       "      <th>í¤ìëê´ê³ </th>\n",
       "      <th>íì´ì¤ë¶</th>\n",
       "      <th>íì´ì¤ë¶ê´ê³ </th>\n",
       "      <th>ííì´ì§ë§ëëë²</th>\n",
       "      <th>ííì´ì§ì ìë°©ë²</th>\n",
       "      <th>ííì´ì§ì ìì¬ì´í¸</th>\n",
       "      <th>ííì´ì§ì ììì²´</th>\n",
       "      <th>ííì´ì§íë¸ë¡ê·¸</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 27756 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaaaaaa  aaaaaaaaaahhhhhhhh  aaaaaaaaargh  aaaaaaaanyway  aardvark  aaron  \\\n",
       "0        0                   0             0              0         0      0   \n",
       "1        0                   0             0              0         0      0   \n",
       "2        0                   0             0              0         0      0   \n",
       "3        0                   0             0              0         0      0   \n",
       "4        0                   0             0              0         0      0   \n",
       "\n",
       "   aawww  ababa  aback  abandon  ...  ì¹´ì¹´ì¤ì¤í ë¦¬  ì¹´í¡mad  í¤ìëê´ê³   íì´ì¤ë¶  íì´ì¤ë¶ê´ê³   \\\n",
       "0      0      0      0        0  ...       0      0      0     0       0   \n",
       "1      0      0      0        0  ...       0      0      0     0       0   \n",
       "2      0      0      0        0  ...       0      0      0     0       0   \n",
       "3      0      0      0        0  ...       0      0      0     0       0   \n",
       "4      0      0      0        0  ...       0      0      0     0       0   \n",
       "\n",
       "   ííì´ì§ë§ëëë²  ííì´ì§ì ìë°©ë²  ííì´ì§ì ìì¬ì´í¸  ííì´ì§ì ììì²´  ííì´ì§íë¸ë¡ê·¸  \n",
       "0         0         0          0         0         0  \n",
       "1         0         0          0         0         0  \n",
       "2         0         0          0         0         0  \n",
       "3         0         0          0         0         0  \n",
       "4         0         0          0         0         0  \n",
       "\n",
       "[5 rows x 27756 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = mens_issues.index\n",
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x11f48cee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwardmendoza/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['don'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words.stop_words)\n",
    "data_cv = cv.fit_transform(mens_issues.all_text)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = mens_issues.index\n",
    "\n",
    "# Pickle it for later use\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"autoerotic\" + 0.004*\"catastoph\" + 0.004*\"coattail\" + 0.004*\"ablat\" + 0.004*\"battlefront\" + 0.003*\"chequ\" + 0.003*\"coars\" + 0.003*\"child\" + 0.003*\"afghanistan\" + 0.003*\"bigfram\"'),\n",
       " (1,\n",
       "  '0.003*\"gail\" + 0.003*\"bugaloo\" + 0.002*\"gona\" + 0.002*\"hatefil\" + 0.002*\"burr\" + 0.002*\"girl\" + 0.002*\"bezo\" + 0.001*\"garth\" + 0.001*\"electromagnet\" + 0.001*\"finest\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10, random_state = 42)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"autoerotic\" + 0.008*\"coars\" + 0.004*\"deathb\" + 0.004*\"chequ\" + 0.004*\"comatos\" + 0.003*\"dehuman\" + 0.003*\"crazyi\" + 0.003*\"catchuphello\" + 0.003*\"cyst\" + 0.003*\"broswol\"'),\n",
       " (1,\n",
       "  '0.023*\"child\" + 0.017*\"autoaggress\" + 0.011*\"catastoph\" + 0.011*\"crowi\" + 0.010*\"contrapoint\" + 0.010*\"alliant\" + 0.008*\"biker\" + 0.008*\"automod\" + 0.008*\"conceptu\" + 0.008*\"dieseas\"'),\n",
       " (2,\n",
       "  '0.026*\"gona\" + 0.014*\"bezo\" + 0.014*\"banish\" + 0.009*\"growingup\" + 0.009*\"girlsll\" + 0.009*\"cipralex\" + 0.006*\"ericport\" + 0.006*\"ball\" + 0.006*\"dinnermovi\" + 0.006*\"awayhr\"'),\n",
       " (3,\n",
       "  '0.014*\"ablat\" + 0.013*\"coattail\" + 0.009*\"catastoph\" + 0.008*\"bing\" + 0.008*\"biomechan\" + 0.008*\"blackpil\" + 0.008*\"close\" + 0.007*\"afghanistan\" + 0.007*\"dissuad\" + 0.007*\"chauffer\"'),\n",
       " (4,\n",
       "  '0.011*\"bugaloo\" + 0.004*\"girl\" + 0.004*\"gail\" + 0.004*\"burr\" + 0.003*\"gona\" + 0.003*\"gang\" + 0.003*\"conveni\" + 0.003*\"beholden\" + 0.003*\"bezo\" + 0.003*\"finest\"'),\n",
       " (5,\n",
       "  '0.006*\"gail\" + 0.003*\"finsta\" + 0.003*\"garth\" + 0.003*\"gamedev\" + 0.002*\"hatefil\" + 0.002*\"haphazardli\" + 0.002*\"friendso\" + 0.002*\"electromagnet\" + 0.002*\"gyneacologist\" + 0.002*\"foolhardi\"'),\n",
       " (6,\n",
       "  '0.052*\"battlefront\" + 0.021*\"chequ\" + 0.020*\"fancentro\" + 0.008*\"chin\" + 0.007*\"burr\" + 0.006*\"catchuphello\" + 0.006*\"devout\" + 0.006*\"curtli\" + 0.005*\"detest\" + 0.005*\"bimek\"'),\n",
       " (7,\n",
       "  '0.004*\"bestclos\" + 0.003*\"carriag\" + 0.003*\"excus\" + 0.003*\"hasslefre\" + 0.003*\"tommi\" + 0.003*\"defenc\" + 0.002*\"fatigu\" + 0.002*\"catchuphello\" + 0.002*\"churchil\" + 0.002*\"fisherman\"'),\n",
       " (8,\n",
       "  '0.008*\"bloodbath\" + 0.007*\"argumentfight\" + 0.006*\"crazyi\" + 0.006*\"broswol\" + 0.005*\"creepth\" + 0.005*\"anarchosyndicalist\" + 0.004*\"burglari\" + 0.004*\"afsp\" + 0.004*\"antibioticresist\" + 0.004*\"blueberri\"'),\n",
       " (9,\n",
       "  '0.014*\"cowardlyhidden\" + 0.013*\"bougi\" + 0.010*\"fetch\" + 0.009*\"caronli\" + 0.008*\"copingd\" + 0.008*\"gtbrain\" + 0.008*\"friendsformerli\" + 0.008*\"anonymouslylik\" + 0.007*\"aloof\" + 0.007*\"dunham\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda2 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=5, random_state = 42)\n",
    "lda2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"coattail\" + 0.005*\"ablat\" + 0.005*\"autoerotic\" + 0.004*\"coars\" + 0.004*\"catastoph\" + 0.004*\"afghanistan\" + 0.003*\"bigfram\" + 0.003*\"cismenmen\" + 0.003*\"broswol\" + 0.003*\"blackpil\"'),\n",
       " (1,\n",
       "  '0.025*\"battlefront\" + 0.011*\"child\" + 0.009*\"fancentro\" + 0.007*\"banish\" + 0.003*\"biker\" + 0.003*\"earthsea\" + 0.003*\"mannslib\" + 0.003*\"detest\" + 0.003*\"biiter\" + 0.003*\"acept\"'),\n",
       " (2,\n",
       "  '0.008*\"gona\" + 0.004*\"growingup\" + 0.003*\"bezo\" + 0.003*\"gamedev\" + 0.003*\"haphazardli\" + 0.003*\"cipralex\" + 0.003*\"girlsll\" + 0.003*\"dwarfism\" + 0.003*\"nonconsentu\" + 0.003*\"ericport\"'),\n",
       " (3,\n",
       "  '0.008*\"catchuphello\" + 0.007*\"chequ\" + 0.006*\"autoerotic\" + 0.006*\"bugaloo\" + 0.005*\"catastoph\" + 0.004*\"cuddli\" + 0.003*\"argumentfight\" + 0.003*\"anarchosyndicalist\" + 0.003*\"centauri\" + 0.003*\"datingand\"'),\n",
       " (4,\n",
       "  '0.004*\"gail\" + 0.004*\"bugaloo\" + 0.002*\"burr\" + 0.002*\"hatefil\" + 0.002*\"girl\" + 0.002*\"finest\" + 0.002*\"homework\" + 0.002*\"electromagnet\" + 0.002*\"garth\" + 0.002*\"foolhardi\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### passing the model with lower passes, increased num_topics\n",
    "lda3 = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=5, random_state = 42)\n",
    "lda3.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these passes of topic modeling via LDA, it's difficult to interpret the topics that we have here. Will adjust the words that will be passed through the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits to Alice Zhao\n",
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mens_issues['all_text'] = mens_issues.all_text.apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words.stop_words)\n",
    "data_cvn = cvn.fit_transform(mens_issues.all_text)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = mens_issues.all_text.index\n",
    "#data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"friend\" + 0.020*\"girl\" + 0.020*\"date\" + 0.017*\"work\" + 0.016*\"relationship\" + 0.014*\"life\" + 0.011*\"help\" + 0.009*\"start\" + 0.008*\"look\" + 0.006*\"school\"'),\n",
       " (1,\n",
       "  '0.040*\"women\" + 0.017*\"woman\" + 0.013*\"delet\" + 0.007*\"male\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.006*\"look\" + 0.005*\"attract\" + 0.005*\"gener\" + 0.005*\"issu\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10, random_state = 42)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"friend\" + 0.020*\"girl\" + 0.020*\"date\" + 0.017*\"work\" + 0.016*\"relationship\" + 0.014*\"life\" + 0.011*\"help\" + 0.009*\"start\" + 0.008*\"look\" + 0.006*\"school\"'),\n",
       " (1,\n",
       "  '0.040*\"women\" + 0.017*\"woman\" + 0.013*\"delet\" + 0.007*\"male\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.006*\"look\" + 0.005*\"attract\" + 0.005*\"gener\" + 0.005*\"issu\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 2\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10, random_state = 42)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"friend\" + 0.020*\"life\" + 0.019*\"work\" + 0.014*\"help\" + 0.014*\"relationship\" + 0.012*\"start\" + 0.008*\"school\" + 0.007*\"love\" + 0.006*\"look\" + 0.006*\"doesnt\"'),\n",
       " (1,\n",
       "  '0.088*\"date\" + 0.077*\"women\" + 0.041*\"girl\" + 0.035*\"woman\" + 0.023*\"look\" + 0.017*\"attract\" + 0.014*\"relationship\" + 0.011*\"number\" + 0.009*\"approach\" + 0.008*\"differ\"'),\n",
       " (2,\n",
       "  '0.030*\"delet\" + 0.016*\"masculin\" + 0.015*\"male\" + 0.010*\"help\" + 0.009*\"gender\" + 0.007*\"health\" + 0.007*\"emot\" + 0.007*\"posit\" + 0.007*\"rape\" + 0.006*\"toxic\"'),\n",
       " (3,\n",
       "  '0.077*\"girl\" + 0.022*\"relationship\" + 0.020*\"partner\" + 0.019*\"boyfriend\" + 0.014*\"cheat\" + 0.013*\"hook\" + 0.013*\"dick\" + 0.012*\"girlfriend\" + 0.012*\"tell\" + 0.011*\"drink\"'),\n",
       " (4,\n",
       "  '0.014*\"work\" + 0.011*\"hair\" + 0.010*\"issu\" + 0.008*\"place\" + 0.008*\"group\" + 0.007*\"favorit\" + 0.007*\"commun\" + 0.006*\"product\" + 0.006*\"space\" + 0.005*\"look\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3, increasing num_topics and passes\n",
    "ldan2 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15, random_state = 42)\n",
    "ldan2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.028*\"friend\" + 0.020*\"life\" + 0.019*\"work\" + 0.014*\"help\" + 0.014*\"relationship\" + 0.012*\"start\" + 0.008*\"school\" + 0.007*\"love\" + 0.006*\"look\" + 0.006*\"doesnt\"'),\n",
       " (1,\n",
       "  '0.088*\"date\" + 0.077*\"women\" + 0.041*\"girl\" + 0.035*\"woman\" + 0.023*\"look\" + 0.017*\"attract\" + 0.014*\"relationship\" + 0.011*\"number\" + 0.009*\"approach\" + 0.008*\"differ\"'),\n",
       " (2,\n",
       "  '0.030*\"delet\" + 0.016*\"masculin\" + 0.015*\"male\" + 0.010*\"help\" + 0.009*\"gender\" + 0.007*\"health\" + 0.007*\"emot\" + 0.007*\"posit\" + 0.007*\"rape\" + 0.006*\"toxic\"'),\n",
       " (3,\n",
       "  '0.077*\"girl\" + 0.022*\"relationship\" + 0.020*\"partner\" + 0.019*\"boyfriend\" + 0.014*\"cheat\" + 0.013*\"hook\" + 0.013*\"dick\" + 0.012*\"girlfriend\" + 0.012*\"tell\" + 0.011*\"drink\"'),\n",
       " (4,\n",
       "  '0.014*\"work\" + 0.011*\"hair\" + 0.010*\"issu\" + 0.008*\"place\" + 0.008*\"group\" + 0.007*\"favorit\" + 0.007*\"commun\" + 0.006*\"product\" + 0.006*\"space\" + 0.005*\"look\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 4, increasing num_topics and passes\n",
    "ldan3 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=15, random_state = 42)\n",
    "ldan3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"friend\" + 0.020*\"girl\" + 0.019*\"date\" + 0.017*\"work\" + 0.016*\"relationship\" + 0.015*\"life\" + 0.011*\"help\" + 0.009*\"start\" + 0.008*\"look\" + 0.006*\"school\"'),\n",
       " (1,\n",
       "  '0.038*\"women\" + 0.017*\"woman\" + 0.012*\"delet\" + 0.007*\"male\" + 0.007*\"attract\" + 0.007*\"hair\" + 0.006*\"partner\" + 0.006*\"look\" + 0.005*\"masculin\" + 0.005*\"gener\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 5\n",
    "ldan4 = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"friend\" + 0.019*\"life\" + 0.019*\"work\" + 0.016*\"relationship\" + 0.012*\"start\" + 0.012*\"help\" + 0.008*\"school\" + 0.007*\"girl\" + 0.007*\"love\" + 0.007*\"girlfriend\"'),\n",
       " (1,\n",
       "  '0.083*\"date\" + 0.076*\"women\" + 0.033*\"woman\" + 0.031*\"girl\" + 0.022*\"look\" + 0.017*\"attract\" + 0.011*\"number\" + 0.010*\"approach\" + 0.008*\"differ\" + 0.008*\"kind\"'),\n",
       " (2,\n",
       "  '0.040*\"delet\" + 0.020*\"help\" + 0.014*\"masculin\" + 0.013*\"male\" + 0.008*\"emot\" + 0.008*\"health\" + 0.008*\"rape\" + 0.007*\"need\" + 0.007*\"peni\" + 0.006*\"posit\"'),\n",
       " (3,\n",
       "  '0.071*\"girl\" + 0.029*\"relationship\" + 0.020*\"partner\" + 0.019*\"boyfriend\" + 0.013*\"cheat\" + 0.013*\"dick\" + 0.012*\"hook\" + 0.011*\"tell\" + 0.011*\"drink\" + 0.011*\"virgin\"'),\n",
       " (4,\n",
       "  '0.017*\"work\" + 0.014*\"hair\" + 0.008*\"issu\" + 0.007*\"turn\" + 0.007*\"place\" + 0.006*\"favorit\" + 0.006*\"look\" + 0.006*\"commun\" + 0.006*\"product\" + 0.006*\"group\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 6\n",
    "ldan5 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=5, random_state = 42)\n",
    "ldan5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*\"friend\" + 0.020*\"life\" + 0.019*\"work\" + 0.015*\"relationship\" + 0.013*\"help\" + 0.012*\"start\" + 0.008*\"school\" + 0.007*\"love\" + 0.006*\"look\" + 0.006*\"doesnt\"'),\n",
       " (1,\n",
       "  '0.087*\"date\" + 0.077*\"women\" + 0.038*\"girl\" + 0.035*\"woman\" + 0.023*\"look\" + 0.017*\"attract\" + 0.012*\"relationship\" + 0.011*\"number\" + 0.009*\"approach\" + 0.008*\"differ\"'),\n",
       " (2,\n",
       "  '0.034*\"delet\" + 0.017*\"help\" + 0.015*\"masculin\" + 0.015*\"male\" + 0.008*\"health\" + 0.008*\"emot\" + 0.008*\"posit\" + 0.007*\"rape\" + 0.007*\"gender\" + 0.006*\"issu\"'),\n",
       " (3,\n",
       "  '0.078*\"girl\" + 0.023*\"relationship\" + 0.020*\"partner\" + 0.020*\"boyfriend\" + 0.014*\"cheat\" + 0.013*\"hook\" + 0.013*\"dick\" + 0.012*\"tell\" + 0.011*\"drink\" + 0.011*\"virgin\"'),\n",
       " (4,\n",
       "  '0.014*\"work\" + 0.012*\"hair\" + 0.009*\"issu\" + 0.008*\"place\" + 0.007*\"group\" + 0.007*\"favorit\" + 0.007*\"commun\" + 0.006*\"product\" + 0.006*\"space\" + 0.006*\"turn\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 7\n",
    "ldan6 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10, random_state = 42)\n",
    "ldan6.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.029*\"friend\" + 0.020*\"life\" + 0.019*\"work\" + 0.015*\"relationship\" + 0.013*\"help\" + 0.012*\"start\" + 0.008*\"school\" + 0.007*\"love\" + 0.006*\"look\" + 0.006*\"doesnt\"'),\n",
       " (1,\n",
       "  '0.087*\"date\" + 0.077*\"women\" + 0.038*\"girl\" + 0.035*\"woman\" + 0.023*\"look\" + 0.017*\"attract\" + 0.012*\"relationship\" + 0.011*\"number\" + 0.009*\"approach\" + 0.008*\"differ\"'),\n",
       " (2,\n",
       "  '0.034*\"delet\" + 0.017*\"help\" + 0.015*\"masculin\" + 0.015*\"male\" + 0.008*\"health\" + 0.008*\"emot\" + 0.008*\"posit\" + 0.007*\"rape\" + 0.007*\"gender\" + 0.006*\"issu\"'),\n",
       " (3,\n",
       "  '0.078*\"girl\" + 0.023*\"relationship\" + 0.020*\"partner\" + 0.020*\"boyfriend\" + 0.014*\"cheat\" + 0.013*\"hook\" + 0.013*\"dick\" + 0.012*\"tell\" + 0.011*\"drink\" + 0.011*\"virgin\"'),\n",
       " (4,\n",
       "  '0.014*\"work\" + 0.012*\"hair\" + 0.009*\"issu\" + 0.008*\"place\" + 0.007*\"group\" + 0.007*\"favorit\" + 0.007*\"commun\" + 0.006*\"product\" + 0.006*\"space\" + 0.006*\"turn\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 8\n",
    "ldan7 = models.LdaModel(corpus=corpusn, num_topics=5, id2word=id2wordn, passes=10, random_state = 42)\n",
    "ldan7.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Attempt#3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pull out nuns and adjectives \n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_nouns_adj = pd.DataFrame(mens_issues.all_text.apply(nouns_adj))\n",
    "#data_nouns_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "\n",
    "cvna = CountVectorizer(stop_words=stop_words.stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.all_text)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "#data_dtmna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"women\" + 0.018*\"woman\" + 0.008*\"delet\" + 0.008*\"male\" + 0.007*\"hair\" + 0.006*\"masculin\" + 0.005*\"issu\" + 0.005*\"partner\" + 0.005*\"bodi\" + 0.005*\"gener\"'),\n",
       " (1,\n",
       "  '0.022*\"date\" + 0.022*\"girl\" + 0.019*\"friend\" + 0.018*\"relationship\" + 0.018*\"work\" + 0.016*\"life\" + 0.011*\"help\" + 0.008*\"start\" + 0.007*\"look\" + 0.007*\"school\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10, random_state = 42)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"hair\" + 0.019*\"bodi\" + 0.016*\"look\" + 0.010*\"masturb\" + 0.009*\"face\" + 0.008*\"studi\" + 0.008*\"size\" + 0.007*\"dick\" + 0.006*\"babi\" + 0.005*\"product\"'),\n",
       " (1,\n",
       "  '0.052*\"date\" + 0.051*\"girl\" + 0.041*\"friend\" + 0.038*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"text\" + 0.008*\"advic\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.084*\"women\" + 0.016*\"woman\" + 0.014*\"masculin\" + 0.012*\"male\" + 0.010*\"gender\" + 0.008*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"discuss\" + 0.006*\"group\"'),\n",
       " (3,\n",
       "  '0.031*\"life\" + 0.029*\"work\" + 0.015*\"help\" + 0.011*\"school\" + 0.009*\"delet\" + 0.009*\"start\" + 0.008*\"home\" + 0.008*\"problem\" + 0.008*\"parent\" + 0.008*\"depress\"'),\n",
       " (4,\n",
       "  '0.025*\"partner\" + 0.024*\"help\" + 0.016*\"relationship\" + 0.011*\"need\" + 0.011*\"mother\" + 0.011*\"father\" + 0.010*\"children\" + 0.009*\"wife\" + 0.009*\"abus\" + 0.009*\"emot\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass2- Let's try 5 topics, 20 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=20, random_state = 42)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"bodi\" + 0.020*\"hair\" + 0.015*\"look\" + 0.009*\"masturb\" + 0.009*\"studi\" + 0.008*\"face\" + 0.008*\"size\" + 0.008*\"dick\" + 0.006*\"babi\" + 0.005*\"product\"'),\n",
       " (1,\n",
       "  '0.051*\"date\" + 0.051*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"text\" + 0.008*\"advic\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.081*\"women\" + 0.015*\"masculin\" + 0.015*\"woman\" + 0.012*\"male\" + 0.011*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"discuss\" + 0.006*\"group\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.028*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.009*\"delet\" + 0.008*\"problem\" + 0.008*\"home\" + 0.008*\"parent\" + 0.008*\"depress\"'),\n",
       " (4,\n",
       "  '0.024*\"partner\" + 0.024*\"help\" + 0.015*\"relationship\" + 0.011*\"mother\" + 0.011*\"father\" + 0.011*\"need\" + 0.010*\"abus\" + 0.010*\"children\" + 0.009*\"wife\" + 0.009*\"emot\"')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3 - Let's try 5 topics, 30 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=30, random_state = 42)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"bodi\" + 0.019*\"hair\" + 0.015*\"look\" + 0.009*\"masturb\" + 0.009*\"studi\" + 0.008*\"face\" + 0.008*\"size\" + 0.008*\"dick\" + 0.006*\"babi\" + 0.005*\"product\"'),\n",
       " (1,\n",
       "  '0.050*\"date\" + 0.050*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"text\" + 0.008*\"advic\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.078*\"women\" + 0.016*\"masculin\" + 0.014*\"woman\" + 0.012*\"male\" + 0.011*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"discuss\" + 0.006*\"group\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.028*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.008*\"problem\" + 0.008*\"delet\" + 0.008*\"depress\" + 0.008*\"parent\" + 0.008*\"home\"'),\n",
       " (4,\n",
       "  '0.024*\"partner\" + 0.023*\"help\" + 0.015*\"relationship\" + 0.012*\"mother\" + 0.011*\"father\" + 0.011*\"need\" + 0.010*\"abus\" + 0.010*\"children\" + 0.009*\"rape\" + 0.009*\"emot\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 3 - Let's try 5 topics, 40 passes\n",
    "ldana2 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=40, random_state = 42)\n",
    "ldana2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"bodi\" + 0.019*\"hair\" + 0.015*\"look\" + 0.010*\"studi\" + 0.009*\"masturb\" + 0.008*\"face\" + 0.008*\"size\" + 0.007*\"dick\" + 0.006*\"babi\" + 0.005*\"research\"'),\n",
       " (1,\n",
       "  '0.050*\"date\" + 0.050*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"text\" + 0.008*\"advic\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.077*\"women\" + 0.017*\"masculin\" + 0.014*\"woman\" + 0.012*\"male\" + 0.011*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"group\" + 0.007*\"discuss\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.027*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.008*\"problem\" + 0.008*\"depress\" + 0.008*\"parent\" + 0.008*\"delet\" + 0.008*\"home\"'),\n",
       " (4,\n",
       "  '0.024*\"partner\" + 0.023*\"help\" + 0.015*\"relationship\" + 0.012*\"mother\" + 0.011*\"father\" + 0.011*\"abus\" + 0.010*\"need\" + 0.010*\"rape\" + 0.010*\"children\" + 0.009*\"emot\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 4 - Let's try 5 topics, 50 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=50, random_state = 42)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"hair\" + 0.020*\"bodi\" + 0.015*\"look\" + 0.010*\"masturb\" + 0.009*\"studi\" + 0.008*\"face\" + 0.008*\"size\" + 0.008*\"dick\" + 0.006*\"babi\" + 0.005*\"product\"'),\n",
       " (1,\n",
       "  '0.051*\"date\" + 0.051*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"text\" + 0.008*\"advic\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.082*\"women\" + 0.015*\"woman\" + 0.015*\"masculin\" + 0.012*\"male\" + 0.010*\"gender\" + 0.008*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"discuss\" + 0.006*\"group\"'),\n",
       " (3,\n",
       "  '0.031*\"life\" + 0.029*\"work\" + 0.015*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.009*\"delet\" + 0.008*\"home\" + 0.008*\"problem\" + 0.008*\"parent\" + 0.008*\"depress\"'),\n",
       " (4,\n",
       "  '0.024*\"partner\" + 0.024*\"help\" + 0.015*\"relationship\" + 0.011*\"mother\" + 0.011*\"need\" + 0.011*\"father\" + 0.010*\"children\" + 0.010*\"abus\" + 0.009*\"wife\" + 0.009*\"emot\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 5 - Let's try 5 topics, 25 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=25, random_state = 42)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"delet\" + 0.021*\"hair\" + 0.021*\"bodi\" + 0.017*\"look\" + 0.011*\"dick\" + 0.010*\"masturb\" + 0.009*\"face\" + 0.009*\"size\" + 0.007*\"peni\" + 0.007*\"studi\"'),\n",
       " (1,\n",
       "  '0.057*\"date\" + 0.057*\"girl\" + 0.048*\"relationship\" + 0.042*\"friend\" + 0.013*\"woman\" + 0.010*\"look\" + 0.009*\"text\" + 0.009*\"girlfriend\" + 0.008*\"start\" + 0.008*\"advic\"'),\n",
       " (2,\n",
       "  '0.085*\"women\" + 0.018*\"masculin\" + 0.012*\"male\" + 0.012*\"gender\" + 0.010*\"woman\" + 0.010*\"issu\" + 0.009*\"posit\" + 0.008*\"group\" + 0.008*\"commun\" + 0.008*\"discuss\"'),\n",
       " (3,\n",
       "  '0.018*\"life\" + 0.016*\"help\" + 0.012*\"depress\" + 0.012*\"problem\" + 0.010*\"health\" + 0.008*\"watch\" + 0.008*\"point\" + 0.007*\"hate\" + 0.007*\"look\" + 0.007*\"kind\"'),\n",
       " (4,\n",
       "  '0.039*\"partner\" + 0.034*\"help\" + 0.017*\"abus\" + 0.016*\"need\" + 0.016*\"rape\" + 0.011*\"victim\" + 0.011*\"assault\" + 0.010*\"children\" + 0.010*\"emot\" + 0.009*\"want\"'),\n",
       " (5,\n",
       "  '0.053*\"work\" + 0.032*\"life\" + 0.017*\"school\" + 0.013*\"parent\" + 0.013*\"friend\" + 0.013*\"home\" + 0.012*\"help\" + 0.010*\"famili\" + 0.010*\"start\" + 0.009*\"colleg\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass 6 - Let's try 6 topics, 50 passes\n",
    "ldana5 = models.LdaModel(corpus=corpusna, num_topics=6, id2word=id2wordna, passes=50, random_state = 42)\n",
    "ldana5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"woman\" + 0.030*\"hair\" + 0.026*\"look\" + 0.024*\"bodi\" + 0.014*\"face\" + 0.010*\"care\" + 0.008*\"product\" + 0.008*\"turn\" + 0.007*\"dress\" + 0.007*\"male\"'),\n",
       " (1,\n",
       "  '0.077*\"date\" + 0.076*\"girl\" + 0.052*\"relationship\" + 0.020*\"friend\" + 0.012*\"text\" + 0.012*\"woman\" + 0.011*\"look\" + 0.010*\"attract\" + 0.010*\"number\" + 0.010*\"advic\"'),\n",
       " (2,\n",
       "  '0.096*\"women\" + 0.016*\"masculin\" + 0.013*\"male\" + 0.012*\"gender\" + 0.009*\"issu\" + 0.009*\"posit\" + 0.008*\"discuss\" + 0.008*\"gener\" + 0.008*\"favorit\" + 0.008*\"rule\"'),\n",
       " (3,\n",
       "  '0.016*\"watch\" + 0.014*\"dick\" + 0.012*\"masturb\" + 0.011*\"virgin\" + 0.009*\"porn\" + 0.008*\"compliment\" + 0.008*\"peni\" + 0.007*\"hand\" + 0.006*\"penis\" + 0.005*\"video\"'),\n",
       " (4,\n",
       "  '0.054*\"delet\" + 0.038*\"partner\" + 0.028*\"help\" + 0.024*\"relationship\" + 0.019*\"cheat\" + 0.016*\"need\" + 0.013*\"rape\" + 0.011*\"wife\" + 0.010*\"abus\" + 0.010*\"woman\"'),\n",
       " (5,\n",
       "  '0.115*\"friend\" + 0.016*\"girlfriend\" + 0.016*\"weekend\" + 0.016*\"group\" + 0.015*\"deal\" + 0.011*\"friendship\" + 0.011*\"plan\" + 0.010*\"parent\" + 0.008*\"famili\" + 0.008*\"hang\"'),\n",
       " (6,\n",
       "  '0.031*\"work\" + 0.030*\"life\" + 0.016*\"help\" + 0.013*\"school\" + 0.011*\"start\" + 0.008*\"home\" + 0.008*\"problem\" + 0.008*\"care\" + 0.008*\"depress\" + 0.007*\"place\"')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 7 topics, 20 passes\n",
    "ldana3 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=20, random_state = 42)\n",
    "ldana3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"woman\" + 0.029*\"hair\" + 0.025*\"bodi\" + 0.025*\"look\" + 0.013*\"face\" + 0.010*\"care\" + 0.007*\"product\" + 0.007*\"male\" + 0.007*\"turn\" + 0.007*\"dress\"'),\n",
       " (1,\n",
       "  '0.075*\"date\" + 0.075*\"girl\" + 0.052*\"relationship\" + 0.021*\"friend\" + 0.012*\"text\" + 0.012*\"woman\" + 0.011*\"look\" + 0.010*\"attract\" + 0.010*\"number\" + 0.009*\"advic\"'),\n",
       " (2,\n",
       "  '0.091*\"women\" + 0.017*\"masculin\" + 0.013*\"male\" + 0.012*\"gender\" + 0.010*\"issu\" + 0.009*\"posit\" + 0.008*\"discuss\" + 0.008*\"gener\" + 0.007*\"role\" + 0.007*\"group\"'),\n",
       " (3,\n",
       "  '0.016*\"watch\" + 0.014*\"dick\" + 0.012*\"masturb\" + 0.011*\"virgin\" + 0.009*\"porn\" + 0.008*\"peni\" + 0.008*\"compliment\" + 0.007*\"hand\" + 0.006*\"video\" + 0.005*\"joke\"'),\n",
       " (4,\n",
       "  '0.051*\"delet\" + 0.038*\"partner\" + 0.029*\"help\" + 0.022*\"relationship\" + 0.018*\"cheat\" + 0.015*\"need\" + 0.014*\"rape\" + 0.011*\"abus\" + 0.010*\"woman\" + 0.010*\"wife\"'),\n",
       " (5,\n",
       "  '0.115*\"friend\" + 0.016*\"group\" + 0.016*\"girlfriend\" + 0.015*\"deal\" + 0.015*\"weekend\" + 0.012*\"friendship\" + 0.011*\"parent\" + 0.011*\"plan\" + 0.008*\"famili\" + 0.008*\"hang\"'),\n",
       " (6,\n",
       "  '0.030*\"work\" + 0.030*\"life\" + 0.016*\"help\" + 0.013*\"school\" + 0.011*\"start\" + 0.008*\"problem\" + 0.008*\"home\" + 0.008*\"depress\" + 0.008*\"care\" + 0.007*\"place\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 7 topics, 30 passes\n",
    "ldana4 = models.LdaModel(corpus=corpusna, num_topics=7, id2word=id2wordna, passes=30, random_state = 42)\n",
    "ldana4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"hair\" + 0.017*\"bodi\" + 0.014*\"look\" + 0.010*\"dick\" + 0.009*\"masturb\" + 0.007*\"face\" + 0.007*\"size\" + 0.007*\"woman\" + 0.006*\"watch\" + 0.006*\"porn\"'),\n",
       " (1,\n",
       "  '0.054*\"date\" + 0.054*\"girl\" + 0.046*\"relationship\" + 0.040*\"friend\" + 0.014*\"delet\" + 0.012*\"woman\" + 0.009*\"girlfriend\" + 0.009*\"text\" + 0.009*\"look\" + 0.008*\"partner\"'),\n",
       " (2,\n",
       "  '0.073*\"women\" + 0.013*\"male\" + 0.012*\"masculin\" + 0.009*\"woman\" + 0.009*\"gender\" + 0.008*\"issu\" + 0.007*\"discuss\" + 0.007*\"posit\" + 0.006*\"gener\" + 0.005*\"favorit\"'),\n",
       " (3,\n",
       "  '0.025*\"life\" + 0.025*\"work\" + 0.019*\"help\" + 0.011*\"school\" + 0.008*\"start\" + 0.007*\"problem\" + 0.007*\"home\" + 0.007*\"parent\" + 0.006*\"depress\" + 0.006*\"place\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5th Attempt, 20 passes, 4 topics\n",
    "ldana5 = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=20, random_state = 42)\n",
    "ldana5.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"bodi\" + 0.019*\"hair\" + 0.015*\"look\" + 0.010*\"studi\" + 0.009*\"masturb\" + 0.008*\"size\" + 0.008*\"face\" + 0.007*\"dick\" + 0.006*\"research\" + 0.006*\"babi\"'),\n",
       " (1,\n",
       "  '0.049*\"date\" + 0.049*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"advic\" + 0.008*\"text\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.074*\"women\" + 0.018*\"masculin\" + 0.013*\"woman\" + 0.012*\"male\" + 0.012*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"role\" + 0.007*\"group\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.027*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.008*\"depress\" + 0.008*\"problem\" + 0.008*\"parent\" + 0.008*\"home\" + 0.007*\"delet\"'),\n",
       " (4,\n",
       "  '0.023*\"partner\" + 0.022*\"help\" + 0.015*\"relationship\" + 0.012*\"mother\" + 0.011*\"father\" + 0.011*\"abus\" + 0.010*\"rape\" + 0.010*\"emot\" + 0.010*\"need\" + 0.009*\"children\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6th Attempt, 80 passes, 5 topics\n",
    "ldana6 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=80, random_state = 42)\n",
    "ldana6.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"bodi\" + 0.018*\"hair\" + 0.015*\"look\" + 0.010*\"studi\" + 0.009*\"masturb\" + 0.008*\"size\" + 0.008*\"face\" + 0.007*\"dick\" + 0.006*\"research\" + 0.006*\"particip\"'),\n",
       " (1,\n",
       "  '0.049*\"date\" + 0.049*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"advic\" + 0.008*\"text\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.073*\"women\" + 0.018*\"masculin\" + 0.013*\"woman\" + 0.012*\"male\" + 0.012*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"role\" + 0.007*\"group\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.027*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.009*\"depress\" + 0.008*\"problem\" + 0.008*\"parent\" + 0.008*\"home\" + 0.007*\"delet\"'),\n",
       " (4,\n",
       "  '0.023*\"partner\" + 0.022*\"help\" + 0.015*\"relationship\" + 0.012*\"mother\" + 0.011*\"father\" + 0.011*\"abus\" + 0.010*\"rape\" + 0.010*\"emot\" + 0.010*\"need\" + 0.009*\"children\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7th Attempt, 90 passes, 5 topics\n",
    "ldana7 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=90, random_state = 42)\n",
    "ldana7.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"bodi\" + 0.018*\"hair\" + 0.015*\"look\" + 0.010*\"studi\" + 0.009*\"masturb\" + 0.008*\"size\" + 0.008*\"face\" + 0.007*\"dick\" + 0.006*\"research\" + 0.006*\"particip\"'),\n",
       " (1,\n",
       "  '0.049*\"date\" + 0.049*\"girl\" + 0.041*\"friend\" + 0.039*\"relationship\" + 0.010*\"look\" + 0.010*\"woman\" + 0.009*\"girlfriend\" + 0.008*\"advic\" + 0.008*\"text\" + 0.008*\"start\"'),\n",
       " (2,\n",
       "  '0.073*\"women\" + 0.018*\"masculin\" + 0.013*\"woman\" + 0.013*\"male\" + 0.012*\"gender\" + 0.009*\"issu\" + 0.008*\"posit\" + 0.007*\"gener\" + 0.007*\"role\" + 0.007*\"group\"'),\n",
       " (3,\n",
       "  '0.030*\"life\" + 0.027*\"work\" + 0.016*\"help\" + 0.011*\"school\" + 0.009*\"start\" + 0.009*\"depress\" + 0.008*\"problem\" + 0.008*\"parent\" + 0.008*\"home\" + 0.007*\"delet\"'),\n",
       " (4,\n",
       "  '0.023*\"partner\" + 0.022*\"help\" + 0.015*\"relationship\" + 0.012*\"mother\" + 0.011*\"father\" + 0.011*\"abus\" + 0.010*\"rape\" + 0.010*\"emot\" + 0.009*\"need\" + 0.009*\"children\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8th Attempt, 100 passes, 5 topics\n",
    "ldana8 = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=100, random_state = 42)\n",
    "ldana8.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After so many passes, I decided to add additional words to my `stop_words` hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling - Attempt#4 (Nouns and Adjectives added, with additional stopwords I deemed irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stop_words = ['song', 'life', 'look', 'discuss', 'text']\n",
    "stop_words2 = stop_words.stop_words + add_stop_words\n",
    "\n",
    "cvna2 = CountVectorizer(stop_words=stop_words2, max_df=.8)\n",
    "data_cvna2 = cvna2.fit_transform(data_nouns_adj.all_text)\n",
    "data_dtmna2 = pd.DataFrame(data_cvna2.toarray(), columns=cvna2.get_feature_names())\n",
    "data_dtmna2.index = data_nouns_adj.index\n",
    "#data_dtmna2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna2 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna2.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna2 = dict((v, k) for k, v in cvna2.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"work\" + 0.014*\"parent\" + 0.011*\"money\" + 0.010*\"school\" + 0.010*\"home\" + 0.008*\"wife\" + 0.006*\"famili\" + 0.006*\"mother\" + 0.006*\"father\" + 0.006*\"room\"'),\n",
       " (1,\n",
       "  '0.057*\"delet\" + 0.033*\"masculin\" + 0.019*\"bodi\" + 0.017*\"favorit\" + 0.017*\"dick\" + 0.014*\"watch\" + 0.013*\"toxic\" + 0.013*\"size\" + 0.011*\"posit\" + 0.011*\"peni\"'),\n",
       " (2,\n",
       "  '0.041*\"date\" + 0.041*\"girl\" + 0.037*\"friend\" + 0.030*\"relationship\" + 0.013*\"work\" + 0.011*\"start\" + 0.009*\"school\" + 0.008*\"girlfriend\" + 0.006*\"kind\" + 0.006*\"doesnt\"'),\n",
       " (3,\n",
       "  '0.032*\"help\" + 0.010*\"problem\" + 0.009*\"issu\" + 0.009*\"health\" + 0.009*\"thank\" + 0.009*\"depress\" + 0.008*\"hair\" + 0.007*\"partner\" + 0.007*\"emot\" + 0.007*\"relationship\"'),\n",
       " (4,\n",
       "  '0.096*\"women\" + 0.043*\"woman\" + 0.014*\"male\" + 0.010*\"gender\" + 0.009*\"attract\" + 0.009*\"gener\" + 0.007*\"rape\" + 0.007*\"reason\" + 0.006*\"partner\" + 0.005*\"societi\"')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 1\n",
    "ldana_1 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=20, random_state = 42)\n",
    "ldana_1.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"work\" + 0.014*\"parent\" + 0.011*\"money\" + 0.010*\"home\" + 0.010*\"school\" + 0.008*\"wife\" + 0.007*\"famili\" + 0.006*\"mother\" + 0.006*\"father\" + 0.006*\"room\"'),\n",
       " (1,\n",
       "  '0.053*\"delet\" + 0.037*\"masculin\" + 0.019*\"bodi\" + 0.016*\"favorit\" + 0.016*\"dick\" + 0.014*\"toxic\" + 0.013*\"watch\" + 0.012*\"size\" + 0.011*\"posit\" + 0.010*\"peni\"'),\n",
       " (2,\n",
       "  '0.040*\"date\" + 0.040*\"girl\" + 0.037*\"friend\" + 0.030*\"relationship\" + 0.013*\"work\" + 0.012*\"start\" + 0.009*\"school\" + 0.007*\"girlfriend\" + 0.006*\"kind\" + 0.006*\"doesnt\"'),\n",
       " (3,\n",
       "  '0.032*\"help\" + 0.010*\"problem\" + 0.009*\"issu\" + 0.009*\"health\" + 0.009*\"depress\" + 0.008*\"thank\" + 0.007*\"hair\" + 0.007*\"emot\" + 0.007*\"work\" + 0.007*\"partner\"'),\n",
       " (4,\n",
       "  '0.093*\"women\" + 0.040*\"woman\" + 0.015*\"male\" + 0.010*\"gender\" + 0.008*\"gener\" + 0.008*\"attract\" + 0.007*\"rape\" + 0.006*\"reason\" + 0.005*\"societi\" + 0.005*\"partner\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 2\n",
    "ldana_2 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=30, random_state = 42)\n",
    "ldana_2.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"work\" + 0.014*\"parent\" + 0.011*\"money\" + 0.010*\"home\" + 0.010*\"school\" + 0.008*\"wife\" + 0.007*\"famili\" + 0.007*\"mother\" + 0.006*\"father\" + 0.006*\"room\"'),\n",
       " (1,\n",
       "  '0.051*\"delet\" + 0.040*\"masculin\" + 0.019*\"bodi\" + 0.015*\"toxic\" + 0.015*\"dick\" + 0.015*\"favorit\" + 0.013*\"watch\" + 0.012*\"size\" + 0.012*\"posit\" + 0.010*\"peni\"'),\n",
       " (2,\n",
       "  '0.039*\"date\" + 0.039*\"girl\" + 0.037*\"friend\" + 0.029*\"relationship\" + 0.013*\"work\" + 0.012*\"start\" + 0.010*\"school\" + 0.007*\"girlfriend\" + 0.006*\"kind\" + 0.006*\"doesnt\"'),\n",
       " (3,\n",
       "  '0.032*\"help\" + 0.010*\"problem\" + 0.010*\"health\" + 0.009*\"depress\" + 0.009*\"issu\" + 0.008*\"thank\" + 0.007*\"emot\" + 0.007*\"work\" + 0.007*\"hair\" + 0.007*\"relationship\"'),\n",
       " (4,\n",
       "  '0.090*\"women\" + 0.038*\"woman\" + 0.015*\"male\" + 0.011*\"gender\" + 0.008*\"gener\" + 0.008*\"attract\" + 0.007*\"rape\" + 0.006*\"reason\" + 0.005*\"societi\" + 0.005*\"group\"')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 3\n",
    "ldana_3 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=40, random_state = 42)\n",
    "ldana_3.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.061*\"women\" + 0.022*\"woman\" + 0.013*\"masculin\" + 0.012*\"male\" + 0.009*\"gender\" + 0.008*\"issu\" + 0.007*\"posit\" + 0.007*\"gener\" + 0.006*\"attract\" + 0.005*\"group\"'),\n",
       " (1,\n",
       "  '0.031*\"delet\" + 0.018*\"hair\" + 0.012*\"bodi\" + 0.009*\"favorit\" + 0.008*\"wife\" + 0.008*\"watch\" + 0.006*\"love\" + 0.006*\"want\" + 0.006*\"food\" + 0.006*\"birthday\"'),\n",
       " (2,\n",
       "  '0.041*\"partner\" + 0.022*\"help\" + 0.017*\"dick\" + 0.015*\"masturb\" + 0.014*\"rape\" + 0.012*\"watch\" + 0.011*\"think\" + 0.011*\"virgin\" + 0.011*\"peni\" + 0.011*\"porn\"'),\n",
       " (3,\n",
       "  '0.019*\"health\" + 0.011*\"work\" + 0.010*\"help\" + 0.010*\"children\" + 0.009*\"support\" + 0.009*\"thank\" + 0.008*\"studi\" + 0.006*\"money\" + 0.006*\"univers\" + 0.006*\"research\"'),\n",
       " (4,\n",
       "  '0.023*\"date\" + 0.023*\"girl\" + 0.023*\"friend\" + 0.021*\"work\" + 0.021*\"relationship\" + 0.012*\"help\" + 0.010*\"start\" + 0.009*\"school\" + 0.006*\"problem\" + 0.006*\"depress\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 4\n",
    "ldana_4 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=50)\n",
    "ldana_4.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"partner\" + 0.012*\"health\" + 0.010*\"help\" + 0.010*\"support\" + 0.010*\"issu\" + 0.010*\"male\" + 0.009*\"rape\" + 0.007*\"victim\" + 0.006*\"assault\" + 0.006*\"research\"'),\n",
       " (1,\n",
       "  '0.082*\"women\" + 0.034*\"woman\" + 0.018*\"masculin\" + 0.013*\"date\" + 0.013*\"attract\" + 0.012*\"gender\" + 0.012*\"male\" + 0.008*\"gener\" + 0.007*\"toxic\" + 0.006*\"group\"'),\n",
       " (2,\n",
       "  '0.025*\"delet\" + 0.019*\"girlfriend\" + 0.012*\"wife\" + 0.010*\"mother\" + 0.010*\"watch\" + 0.010*\"father\" + 0.009*\"marri\" + 0.009*\"parent\" + 0.009*\"brother\" + 0.008*\"cheat\"'),\n",
       " (3,\n",
       "  '0.024*\"friend\" + 0.023*\"work\" + 0.022*\"girl\" + 0.022*\"relationship\" + 0.020*\"date\" + 0.015*\"help\" + 0.011*\"start\" + 0.010*\"school\" + 0.007*\"problem\" + 0.007*\"point\"'),\n",
       " (4,\n",
       "  '0.019*\"bodi\" + 0.018*\"hair\" + 0.010*\"dick\" + 0.008*\"watch\" + 0.008*\"love\" + 0.007*\"favorit\" + 0.007*\"size\" + 0.007*\"face\" + 0.007*\"posit\" + 0.006*\"peni\"')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take 5\n",
    "#ldana_5 = models.LdaModel(corpus=corpusna2, num_topics=5, id2word=id2wordna2, passes=60)\n",
    "#ldana_5.print_topics()\n",
    "\n",
    "# Commented the code to preserve the results from this mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, I settled on the results for this model, where number of topics were 5, and the number of overall passes through the corpus were 60\n",
    "\n",
    "Topic 1 --> Sexual Trauma\\\n",
    "Topic 2--> Gender Identity/Roles\\\n",
    "Topic 3--> Familial Issues\\\n",
    "Topic 4--> Dating Advice/Relationships\\\n",
    "Topic 5--> Body Issues/Positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
